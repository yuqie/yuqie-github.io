<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.7.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="学习资料参考： 一个完整的机器学习项目.md 原书Github上代码 Oreilly上原书第二版（可以在线阅读） 第一版翻译 简书第一版 简书第二版第二部分 简书《Scikit-Learn、Keras与TensorFlow机器学习实用指南》第一版和第二版对照 练习题答案参考 参考2">
<meta property="og:type" content="article">
<meta property="og:title" content="《Scikit-Learn与TensorFlow机器学习实用指南》-第一部分">
<meta property="og:url" content="http://yoursite.com/2020/08/25/%E3%80%8AScikit-Learn%E4%B8%8ETensorFlow%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%E3%80%8B/index.html">
<meta property="og:site_name" content="Ringinmay&#39;s Blog">
<meta property="og:description" content="学习资料参考： 一个完整的机器学习项目.md 原书Github上代码 Oreilly上原书第二版（可以在线阅读） 第一版翻译 简书第一版 简书第二版第二部分 简书《Scikit-Learn、Keras与TensorFlow机器学习实用指南》第一版和第二版对照 练习题答案参考 参考2">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://github.com/apachecn/hands-on-ml-zh/raw/master/images/chapter_1/1-13.png">
<meta property="og:image" content="https://img-blog.csdn.net/20170824103329707?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemVhbGZvcnk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img-blog.csdn.net/20170824103423352?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemVhbGZvcnk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-4.PNG">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-5.PNG">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-9.PNG">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-6.PNG">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-11.PNG">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E8%A1%A84-1.PNG">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-15.PNG">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-16.PNG">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-17.PNG">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-18.PNG">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-93eea6b5c197bbc8d7be8b4c14e9f8f3.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-4b43b0aee35624cd95b910189b3dc231.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-e4da079f692fe35778bbdf1fdf120d99.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-20.PNG">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-21.PNG">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-23.PNG">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-8524eb1789cf2093cfccc4c297138c7f.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-f2d02eaf32cb7a351989198531c0d12a.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-f2d02eaf32cb7a351989198531c0d12a.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-9f888eddb683fe5f80f87f44bd727b08.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-b9dce96eb3d5a71b28f9f198c28d2d1b.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-5f344a952e29992de54b8cfe645b2d5b.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-83878c91171338902e0fe0fb97a8c47a.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-7694f4a66316e53c8cdd9d9954bd611d.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-6bc68f603b52e51645b4bbd318f8cdfe.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-7b7f9dbfea05c83784f8b85149852f08.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/5-1.jpg">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-8b908429a5b5ee2e519f8caa16f82ee1.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/tb-5-1.jpg">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-7781983bd977537b3c5d060e217ea82a.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-9a84ebda628c391e3046dfc2307e3c85.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-98c822c91ab5af02c383eb03fa5b5446.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-9a84ebda628c391e3046dfc2307e3c85.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-03549015bd48d379883d926e6857b448.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-03549015bd48d379883d926e6857b448.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-6.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-7.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-8.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-9.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-10.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-11.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-12.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-992fd41f053d328db0ca0287eed0e2e9.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-13.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/5-hinge.jpg">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-5.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_6/102">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-32cb6265eb19ce4be37ecf6650ff766a.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_6/104">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_6/108">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_7/7-7.png">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_7/E7-3.png">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_7/7-12.png">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-50eeccb6ef846e2d0af5daef5cab1fa0.gif">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_8/8-4.jpeg">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_8/8-6.jpeg">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_8/8-7.jpeg">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_8/8-8.jpeg">
<meta property="og:image" content="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_8/8-8.jpeg">
<meta property="og:image" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0908.png">
<meta property="og:image" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0909.png">
<meta property="og:image" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0910.png">
<meta property="og:image" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0915.png">
<meta property="og:image" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0916.png">
<meta property="og:image" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0920.png">
<meta property="og:image" content="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0922.png">
<meta property="og:image" content="https://img2018.cnblogs.com/blog/1226410/201904/1226410-20190424112057869-1957699378.png">
<meta property="article:published_time" content="2020-08-25T11:26:00.000Z">
<meta property="article:modified_time" content="2020-09-16T03:16:32.537Z">
<meta property="article:author" content="QQAI">
<meta property="article:tag" content="sklearn">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/apachecn/hands-on-ml-zh/raw/master/images/chapter_1/1-13.png">

<link rel="canonical" href="http://yoursite.com/2020/08/25/%E3%80%8AScikit-Learn%E4%B8%8ETensorFlow%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%E3%80%8B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>《Scikit-Learn与TensorFlow机器学习实用指南》-第一部分 | Ringinmay's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Ringinmay's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Home is behind, the world ahead</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/25/%E3%80%8AScikit-Learn%E4%B8%8ETensorFlow%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97%E3%80%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="QQAI">
      <meta itemprop="description" content="Home is behind, the world ahead">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ringinmay's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《Scikit-Learn与TensorFlow机器学习实用指南》-第一部分
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-25 19:26:00" itemprop="dateCreated datePublished" datetime="2020-08-25T19:26:00+08:00">2020-08-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-16 11:16:32" itemprop="dateModified" datetime="2020-09-16T11:16:32+08:00">2020-09-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" itemprop="url" rel="index">
                    <span itemprop="name">基础知识</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>学习资料参考：</p>
<p><a href="https://www.cntofu.com/book/27/docs/2.%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE.md" target="_blank" rel="noopener">一个完整的机器学习项目.md</a></p>
<p><a href="https://github.com/ageron/handson-ml2" target="_blank" rel="noopener">原书Github上代码</a></p>
<p><a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" target="_blank" rel="noopener">Oreilly上原书第二版（可以在线阅读）</a></p>
<p><a href="https://github.com/apachecn/hands-on-ml-zh/blob/master/docs/1.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88.md" target="_blank" rel="noopener">第一版翻译</a></p>
<p><a href="https://www.jianshu.com/p/3470a6efbe8d" target="_blank" rel="noopener">简书第一版</a></p>
<p><a href="https://www.jianshu.com/p/86626c79814a" target="_blank" rel="noopener">简书第二版第二部分</a></p>
<p><a href="https://www.jianshu.com/p/4a94798f7dcc" target="_blank" rel="noopener">简书《Scikit-Learn、Keras与TensorFlow机器学习实用指南》第一版和第二版对照</a></p>
<p><a href="https://blog.csdn.net/jiaoyangwm/article/details/82387883#%E7%BB%83%E4%B9%A0%E9%A2%987" target="_blank" rel="noopener">练习题答案参考</a> <a href="https://blog.csdn.net/leowinbow/article/details/88581039" target="_blank" rel="noopener">参考2</a></p>
<a id="more"></a>
<h2 id="第01章-机器学习概览"><a href="#第01章-机器学习概览" class="headerlink" title="第01章 机器学习概览"></a>第01章 机器学习概览</h2><ol>
<li><p>os.makedirs(IMAGES_PATH, exist_ok=True)</p>
</li>
<li><p>pd.merge(left=oecd_bli, right=gdp_per_capita, left_index=True, right_index=True)</p>
<p>left_index和right_index：指定是否以索引为参考进行合并</p>
<!--more-->
<h4 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h4><ol>
<li>如何定义机器学习？</li>
</ol>
<blockquote>
<p>机器学习是通过编程让计算机从数据中进行学习的科学（和艺术）。</p>
<p><strong>更广义的概念：</strong></p>
<p>机器学习是让计算机具有学习的能力，无需进行明确编程。 —— 亚瑟·萨缪尔，1959</p>
<p><strong>工程性的概念：</strong></p>
<p>计算机程序利用经验 E 学习任务 T，性能是 P，如果针对任务 T 的性能 P 随着经验 E 不断增长，则称为机器学习。 —— 汤姆·米切尔，1997</p>
</blockquote>
<ol>
<li>机器学习可以解决的四类问题？</li>
</ol>
<blockquote>
<p>分类，回归，聚类，降维</p>
<p>机器学习可以根据训练时监督的量和类型进行分类。主要有四类：监督学习、非监督学习、半监督学习和强化学习。</p>
</blockquote>
<ol>
<li>什么是带标签的训练集？</li>
</ol>
<blockquote>
<p>在监督学习中，用来训练算法的训练数据包含了答案，称为标签。</p>
</blockquote>
<ol>
<li>最常见的两个监督任务是什么？</li>
</ol>
<blockquote>
<ol>
<li>分类：例如垃圾邮件过滤器：用许多带有归类（垃圾邮件或普通邮件）的邮件样本进行训练，过滤器必须还能对新邮件进行分类。</li>
<li>回归：预测目标数值，例如给出一些特征（里程数、车龄、品牌等等）称作预测值，来预测一辆汽车的价格。。要训练这个系统，需要给出大量汽车样本，包括它们的预测值和标签（即，它们的价格）。</li>
</ol>
</blockquote>
<ol>
<li>指出四个常见的非监督任务？</li>
</ol>
<blockquote>
<ol>
<li><strong>聚类算法</strong> ，检测相似访客的分组。假设有一份关于博客访客的大量数据，不告诉算法某个访客属于哪一类：它会自己找出关系，无需帮助。例如，算法可能注意到 40% 的访客是喜欢漫画书的男性，通常是晚上访问，20% 是科幻爱好者，他们是在周末访问等等。如果使用<strong>层次聚类分析</strong> ，它可能还会细分每个分组为更小的组。这可以帮助你为每个分组定位博文。</li>
<li><strong>可视化算法</strong> ：给算法大量复杂的且不加标签的数据，算法输出数据的2D或3D图像。算法会试图保留数据的结构（即尝试保留输入的独立聚类，避免在图像中重叠），这样就可以明白数据是如何组织起来的，也许还能发现隐藏的规律。</li>
<li><strong>降维</strong> ：降维的目的是简化数据、但是不能失去大部分信息。做法之一是合并若干相关的特征。例如，汽车的里程数与车龄高度相关，降维算法就会将它们合并成一个，表示汽车的磨损。这叫做<strong>特征提取</strong> 。</li>
<li><strong>异常检测（anomaly detection）</strong> ：例如，检测异常的信用卡转账以防欺诈，检测制造缺陷，或者在训练之前自动从训练数据集去除异常值。异常检测的系统使用正常值训练的，当它碰到一个新实例，它可以判断这个新实例是像正常值还是异常值。</li>
</ol>
</blockquote>
<ol>
<li>要让一个机器人能在各种未知地形行走，你会采用什么机器学习算法？</li>
</ol>
<blockquote>
<p>强化学习：</p>
</blockquote>
<ol>
<li>要对你的顾客进行分组，你会采用哪类算法？</li>
</ol>
<blockquote>
<p>非监督学习</p>
</blockquote>
<ol>
<li>垃圾邮件检测是监督学习问题，还是非监督学习问题？</li>
</ol>
<blockquote>
<p>监督学习：因为有标签</p>
</blockquote>
<ol>
<li>什么是在线学习系统？</li>
</ol>
<blockquote>
<p>从导入的数据流进行持续学习</p>
<p>在在线学习中，是用数据实例持续地进行训练，可以一次一个或一次几个实例（称为小批量）。每个学习步骤都很快且廉价，所以系统可以动态地学习收到的最新数据</p>
<p><img src="https://github.com/apachecn/hands-on-ml-zh/raw/master/images/chapter_1/1-13.png" alt="img"></p>
</blockquote>
<ol>
<li>什么是核外学习？</li>
</ol>
<blockquote>
<p>在线学习算法也适用于在超大数据集（一台计算机不足以用于存储它）上训练系统（这称作核外学习，<em>out-of-core</em> learning）。算法每次只加载部分数据，用这些数据进行训练，然后重复这个过程，直到使用完所有数据</p>
</blockquote>
<ol>
<li>什么学习算法是用相似度做预测？</li>
</ol>
<blockquote>
<p>基于实例学习：系统先用记忆学习案例，然后使用相似度测量推广到新的例子</p>
<p>例如，垃圾邮件检测器，不仅能标记和已知的垃圾邮件相同的邮件，也要能标记类似垃圾邮件的邮件。需要测量两封邮件的相似性。一个（简单的）相似度测量方法是统计两封邮件包含的相同单词的数量。如果一封邮件含有许多垃圾邮件中的词，就会被标记为垃圾邮件。</p>
</blockquote>
<ol>
<li>模型参数和学习算法的超参数的区别是什么？</li>
</ol>
<blockquote>
<p>学习算法搜寻模型参数值，使代价函数最小</p>
<p>超参数（hyperparameter）是一个学习算法的参数（而不是模型的），控制正则化的度。</p>
</blockquote>
<ol>
<li>基于模型学习的算法搜寻的是什么？最成功的策略是什么？基于模型学习如何做预测？</li>
</ol>
<blockquote>
<p>搜寻使得代价函数（测量线性模型的预测值和训练样本之间的距离差）最小的模型参数</p>
<p>线性回归算法</p>
<p>研究数据-选择模型-用训练数据进行训练（学习算法搜寻模型参数值，使得代价函数最小）-使用模型对新案例进行预测（这称作推断）</p>
</blockquote>
<ol>
<li>机器学习的四个主要挑战是什么？</li>
</ol>
<blockquote>
<p>训练数据量不足</p>
<p>没有代表性的训练数据</p>
<p>低质量数据</p>
<p>不相关的特征</p>
<p>过拟合</p>
<p>欠拟合</p>
</blockquote>
<ol>
<li>如果模型在训练集上表现好，但推广到新实例表现差，问题是什么？给出三个可能的解决方案。</li>
</ol>
<blockquote>
<p>过拟合：利用超参数正则化</p>
<p>不相关特征：</p>
<ul>
<li>特征选择：在所有存在的特征中选取最有用的特征进行训练。</li>
<li>特征提取：组合存在的特征，生成一个更有用的特征（如前面看到的，可以使用降维算法）。</li>
<li>收集新数据创建新特征。</li>
</ul>
<p>低质量数据：如果训练集中的错误、异常值和噪声（错误测量引入的）太多，系统检测出潜在规律的难度就会变大，性能就会降低。</p>
<ul>
<li>如果一些实例是明显的异常值，最好删掉它们或尝试手工修改错误；</li>
<li>如果一些实例缺少特征（比如，你的 5% 的顾客没有说明年龄），你必须决定是否忽略这个属性、忽略这些实例、填入缺失值（比如，年龄中位数），或者训练一个含有这个特征的模型和一个不含有这个特征的模型，等等。</li>
</ul>
</blockquote>
<ol>
<li>什么是测试集，为什么要使用它？</li>
</ol>
<blockquote>
<p>测试集：用来训练模型的数据集</p>
</blockquote>
<ol>
<li>验证集的目的是什么？</li>
</ol>
<blockquote>
<p>验证集：数据中分出来，对模型进行测试的数据集。</p>
<p>评估模型推广到新样本的效果（即对新样本的性能），可以将模型部署到生产环境，观察它的性能。这么做可以，但如果模型的性能很差，就会引起用户抱怨。更好的选项是将数据分成训练集和测试集。用训练集进行训练，用测试集进行测试。对新样本的错误率称作<strong>推广错误（或样本外错误）</strong> ，通过模型对测试集的评估，可以预估这个错误。这个值可以我们模型对新样本的性能。</p>
</blockquote>
<ol>
<li>如果用测试集调节超参数，会发生什么？</li>
</ol>
<blockquote>
<p>过拟合</p>
<p>在测试集上多次测量了推广误差率，调整了模型和超参数，以使模型最适合这个集合。这意味着模型对新数据的性能不会高。</p>
</blockquote>
<ol>
<li>什么是交叉验证，为什么它比验证集好？</li>
</ol>
<blockquote>
<p>交叉验证：训练集分成互补的子集，每个模型用不同的子集训练，再用剩下的子集验证。一旦确定模型类型和超参数，最终的模型使用这些超参数和全部的训练集进行训练，用测试集得到推广误差率。</p>
<p>而使用验证集：用训练集和多个超参数训练多个模型，选择在验证集上有最佳性能的模型和超参数。当对模型满意时，用测试集再做最后一次测试，以得到推广误差率的预估，会“浪费”过多训练数据在验证集上。</p>
</blockquote>
</li>
</ol>
<hr>
<h2 id="第02章-一个完整的机器学习项目"><a href="#第02章-一个完整的机器学习项目" class="headerlink" title="第02章 一个完整的机器学习项目"></a>第02章 一个完整的机器学习项目</h2><p>[Python API：crc32函数 计算CRC校验值](<a href="https://b）" target="_blank" rel="noopener">https://b）</a></p>
<p><a href="https://blog.csdn.net/liuyu60305002/article/details/6307152" target="_blank" rel="noopener">模2运算</a></p>
<p><a href="https://blog.csdn.net/sparkliang/article/details/5671510" target="_blank" rel="noopener">CRC32算法详细推导</a></p>
<blockquote>
<p>crc32用于计算 <em>data</em> 的 CRC (A cyclic redundancy check 32，循环冗余校验) 值。计算的结果是一个 32 位的整数。本质是<strong>模2除法模2运算包括模2加、模2减、模2乘、模2除四种二进制运算，不考虑进位和借位）</strong>， 的余数，采用的除数不同，CRC的类型也就不一样。通常，CRC的除数用生成多项式来表示。</p>
<p>此算法没有加密强度，不应用于身份验证和数字签名。仅<strong>为验证数据的正确性</strong> ，不适合作为通用散列算法。</p>
<h4 id="特点：检错能力极强，开销小等"><a href="#特点：检错能力极强，开销小等" class="headerlink" title="特点：检错能力极强，开销小等"></a>特点：检错能力极强，开销小等</h4><h4 id="本质"><a href="#本质" class="headerlink" title="本质"></a>本质</h4><p>CRC 算法是以 GF(2) 多项式算术为数学基础的， GF(2) 多项式中只有一个变量 x ，其系数也只有 0 和 1 ，比如：</p>
<p>​    1 <em>x^6 + 0</em>x^5 + 1<em>x^4 + 0</em>x^3 + 0<em>x^2 +1</em>x^1 + 1*x^0</p>
<p>​       = x^6 + x^4 + x + 1</p>
<p>加减运算不考虑进位和退位。说白了就是下面的运算规则：</p>
<p>​    0 + 0 = 0    0 - 0 = 0</p>
<p>​    0 + 1 = 1    0 - 1 = 1</p>
<p>​    1 + 0 = 1    1 - 0 = 1</p>
<p>​    1 + 1 = 0    1 - 1 = 0<br>看看这个规则，其实就是一个<strong>异或运算</strong> 。</p>
<p>每个生成多项式的系数只能是 0 或 1 ，因此可以把它转化为二进制形式表示， 比如 g(x)=x^4 + x + 1 ，那么 g(x) 对应的二进制形式就是 10011 ， 于是把 GF(2) 多项式的除法转换成了二进制形式，和普通除法没有区别，只是加减运算没有进位和退位。</p>
<p>比如基于上述规则计算 11010/1001 ，那么商是 11 ，余数就是 1。</p>
<h4 id="CRC-校验的基本过程"><a href="#CRC-校验的基本过程" class="headerlink" title="CRC 校验的基本过程"></a>CRC 校验的基本过程</h4><p>采用 CRC 校验时，发送方和接收方用同一个生成多项式 g(x) ， g(x) 是一个 GF(2) 多项式，并且 g(x) 的首位和最后一位的系数必须为 1 。</p>
<p>CRC 的处理方法是：发送方用发送数据的二进制多项式 t(x) 除以 g(x) ，得到余数 y(x) 作为 CRC 校验码。校验时，以计算的校正结果是否为 0 为据，判断数据帧是否出错。设生成多项式是 r 阶的（最高位是 x^r ）具体步骤如下面的描述。</p>
<p>发送方：</p>
<p>1 ）在发送的 m 位数据的二进制多项式 t(x) 后添加 r 个 0 ，扩张到 m+ r 位，以容纳 r 位的校验码，追加 0 后的二进制多项式为  T(x) ；</p>
<p>2 ）用 T(x) 除以生成多项式 g(x) ，得到 r 位的余数 y(x) ，它就是 CRC 校验码；</p>
<p>3 ）把 y(x) 追加到 t(x) 后面，此时的数据 s(x) 就是包含了 CRC 校验码的待发送字符串；由于 s(x) = t(x) y(x) ，因此 s(x) 肯定能被 g(x) 除尽。</p>
<p>接收方：</p>
<p>1 ）接收数据 n(x) ，这个 n(x) 就是包含了 CRC 校验码的 m+r 位数据；</p>
<p>2 ）计算 n(x) 除以 g(x) ，如果余数为 0 则表示传输过程没有错误，否则表示有错误。从 n(x) 去掉尾部的 r 位数据，得到的就是原始数据。</p>
<p>生成多项式不是随意选择的，以下是一些标准的 CRC 算法的生成多项式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>标准</th>
<th>生成多项式</th>
<th>16 进制表示</th>
</tr>
</thead>
<tbody>
<tr>
<td>CRC12</td>
<td>x^12 + x^11 + x^3 + x^2 + x + 1</td>
<td>0x80F</td>
</tr>
<tr>
<td>CRC16</td>
<td>x^16 + x^15 + x^2 + 1</td>
<td>0x8005</td>
</tr>
<tr>
<td>CRC16-CCITT</td>
<td>x^16 + x^12 + x^5 + 1</td>
<td>0x1021</td>
</tr>
<tr>
<td>CRC32</td>
<td>x^32 + x^26 + x^23 + x^22 + x^16 + x^12 + x^11+ x^10 + x^8 + x^7 + x^5 + x^4 + x^2 + x + 1</td>
<td>0x04C11DB7</td>
</tr>
</tbody>
</table>
</div>
<p>在python 3.0 之后: 返回值永远是无符号数。要在所有的 Python 版本和平台上获得相同的值，请使用 <strong>crc32(data) &amp; 0xffffffff</strong>。</p>
</blockquote>
<p><strong>问题：</strong> </p>
<p>使用本章的房产数据集：</p>
<ol>
<li>尝试一个支持向量机回归器（<code>sklearn.svm.SVR</code>），使用多个超参数，比如<code>kernel=&quot;linear&quot;</code>（多个超参数<code>C</code>值）。现在不用担心这些超参数是什么含义。最佳的<code>SVR</code>预测表现如何？</li>
</ol>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line"></span><br><span class="line">&gt;svm_reg = SVR(kernel=<span class="string">"linear"</span>)</span><br><span class="line">&gt;svm_reg.fit(housing_prepared, housing_labels)</span><br><span class="line">&gt;housing_predictions = svm_reg.predict(housing_prepared)</span><br><span class="line">&gt;svm_mse = mean_squared_error(housing_labels, housing_predictions)</span><br><span class="line">&gt;svm_rmse = np.sqrt(svm_mse)</span><br><span class="line">&gt;svm_rmse <span class="comment"># output: 111094.6308539982</span></span><br></pre></td></tr></table></figure>
<p>结果不好。</p>
</blockquote>
<ol>
<li>尝试用<code>RandomizedSearchCV</code>替换<code>GridSearchCV</code>。</li>
</ol>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line">&gt;<span class="keyword">from</span> scipy.stats <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line">&gt;param_distribs = &#123;</span><br><span class="line">       <span class="string">'n_estimators'</span>: randint(low=<span class="number">1</span>, high=<span class="number">200</span>),</span><br><span class="line">       <span class="string">'max_features'</span>: randint(low=<span class="number">1</span>, high=<span class="number">8</span>),</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">&gt;forest_reg = RandomForestRegressor(random_state=<span class="number">42</span>)</span><br><span class="line">&gt;rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,</span><br><span class="line">                               n_iter=<span class="number">10</span>, cv=<span class="number">5</span>, scoring=<span class="string">'neg_mean_squared_error'</span>, random_state=<span class="number">42</span>)</span><br><span class="line">&gt;rnd_search.fit(housing_prepared, housing_labels)</span><br></pre></td></tr></table></figure>
</blockquote>
<ol>
<li>尝试在准备流水线中添加一个只选择最重要属性的转换器。</li>
</ol>
<blockquote>
<p>​</p>
</blockquote>
<ol>
<li>尝试创建一个单独的可以完成数据准备和最终预测的流水线。</li>
</ol>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;full_pipeline_with_predictor = Pipeline([</span><br><span class="line">       (<span class="string">"preparation"</span>, full_pipeline),</span><br><span class="line">       (<span class="string">"linear"</span>, LinearRegression())</span><br><span class="line">   ])</span><br><span class="line"></span><br><span class="line">&gt;full_pipeline_with_predictor.fit(housing, housing_labels)</span><br><span class="line">&gt;full_pipeline_with_predictor.predict(some_data)</span><br></pre></td></tr></table></figure>
</blockquote>
<ol>
<li>使用<code>GridSearchCV</code>自动探索一些准备过程中的候选项。</li>
</ol>
<h2 id="第03章-分类"><a href="#第03章-分类" class="headerlink" title="第03章 分类"></a>第03章 分类</h2><p><a href="https://blog.csdn.net/weixin_38145317/article/details/79650188" target="_blank" rel="noopener">SVM、SVC、SVR</a> </p>
<h4 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h4><ol>
<li><p>尝试在 MNIST 数据集上建立一个分类器，使它在测试集上的精度超过 97%。提示：<code>KNeighborsClassifier</code>非常适合这个任务。你只需要找出一个好的超参数值（试一下对权重和超参数<code>n_neighbors</code>进行网格搜索）。</p>
<p>​</p>
</li>
<li><p>写一个函数可以是 MNIST 中的图像任意方向移动（上下左右）一个像素。然后，对训练集上的每张图片，复制四个移动后的副本（每个方向一个副本），把它们加到训练集当中去。最后在扩展后的训练集上训练你最好的模型，并且在测试集上测量它的精度。你应该会观察到你的模型会有更好的表现。这种人工扩大训练集的方法叫做数据增强，或者训练集扩张。</p>
<p>​</p>
</li>
<li><p>拿 Titanic 数据集去捣鼓一番。开始这个项目有一个很棒的平台：Kaggle</p>
<p>​</p>
</li>
<li><p>建立一个垃圾邮件分类器（这是一个更有挑战性的练习）：</p>
</li>
</ol>
<ul>
<li>下载垃圾邮件和非垃圾邮件的样例数据。地址是<a href="https://spamassassin.apache.org/publiccorpus/" target="_blank" rel="noopener">Apache SpamAssassin 的公共数据集</a></li>
<li>解压这些数据集，并且熟悉它的数据格式。</li>
<li>将数据集分成训练集和测试集</li>
<li>写一个数据准备的流水线，将每一封邮件转换为特征向量。你的流水线应该将一封邮件转换为一个稀疏向量，对于所有可能的词，这个向量标志哪个词出现了，哪个词没有出现。举例子，如果所有邮件只包含了<code>&quot;Hello&quot;,&quot;How&quot;,&quot;are&quot;, &quot;you&quot;</code>这四个词，那么一封邮件（内容是：<code>&quot;Hello you Hello Hello you&quot;</code>）将会被转换为向量<code>[1, 0, 0, 1]</code>(意思是：<code>&quot;Hello&quot;</code>出现，<code>&quot;How&quot;</code>不出现，<code>&quot;are&quot;</code>不出现，<code>&quot;you&quot;</code>出现)，或者<code>[3, 0, 0, 2]</code>，如果你想数出每个单词出现的次数。</li>
<li>你也许想给你的流水线增加超参数，控制是否剥过邮件头、将邮件转换为小写、去除标点符号、将所有 URL 替换成<code>&quot;URL&quot;</code>，将所有数字替换成<code>&quot;NUMBER&quot;</code>，或者甚至提取词干（比如，截断词尾。有现成的 Python 库可以做到这点）。</li>
<li>然后 尝试几个不同的分类器，看看你可否建立一个很棒的垃圾邮件分类器，同时有着高召回率和高准确率。</li>
</ul>
<h2 id="第04章-训练模型"><a href="#第04章-训练模型" class="headerlink" title="第04章 训练模型"></a>第04章 训练模型</h2><p>线性回归预测模型：</p>
<p>$\hat y = h_{\theta}(x)=\theta ^T\cdot x$</p>
<p>线性回归模型的MSE损失函数:</p>
<p>$MSE(X,h_\theta)=\frac{1}{m}\sum_{i=1}^{n}(\theta^T\cdot x^{(i)}-y^{(i)})^2$</p>
<h4 id="正态方程"><a href="#正态方程" class="headerlink" title="正态方程"></a><strong>正态方程</strong></h4><p>最小化损失函数 $\hat{\theta} =   (X^T \cdot X)^{-1} \cdot X^T \cdot y $</p>
<p>复杂度：需要计算$X^T\cdot X$ 的逆矩阵，是一个n*n的矩阵，运算复杂度大约在$O(n^{24})$ 到$O(n^{3})$之间。</p>
<p>可以numpy直接求解，也可以使用sklearn：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X, y)</span><br><span class="line">lin_reg.intercept_, lin_reg.coef_</span><br></pre></td></tr></table></figure>
<p><code>LinearRegression类基于numpy.linalg.lstsq （least squares）</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=<span class="number">1e-6</span>)</span><br><span class="line">theta_best_svd</span><br></pre></td></tr></table></figure>
<h5 id="pseudoinverse-Moore-Penrose-inverse"><a href="#pseudoinverse-Moore-Penrose-inverse" class="headerlink" title="pseudoinverse (Moore-Penrose inverse)"></a>pseudoinverse (Moore-Penrose inverse)</h5><p>该函数求解的是伪逆矩阵（pseudoinverse），即广义矩阵。其中最著名的伪逆矩阵为摩尔－彭若斯广义逆 A+（Moore–Penrose pseudoinverse)，常应用于求非一致线性方程组的最小范数最小二乘解（最小二乘法），并使得解的形式变得简单。矩阵的摩尔－彭若斯广义逆在实数域和复数域上都是唯一的，并且可以通过奇异值分解求得。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.linalg.pinv(X_b).dot(y)</span><br></pre></td></tr></table></figure>
<p>满足摩尔-彭若斯条件的矩阵G称为矩阵A的摩尔－彭若斯广义逆矩阵，记作A+：</p>
<p><img src="https://img-blog.csdn.net/20170824103329707?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemVhbGZvcnk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="条件"></p>
<p>从摩尔－彭若斯条件出发，彭若斯推导出了摩尔－彭若斯广义逆的一些性质：<br><img src="https://img-blog.csdn.net/20170824103423352?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemVhbGZvcnk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p> 参考：<a href="https://blog.csdn.net/zealfory/article/details/77526815" target="_blank" rel="noopener">https://blog.csdn.net/zealfory/article/details/77526815</a></p>
<h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p>超参数学习率的值决定了步长的大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的，如果学习率太大，你可能使算法发散，函数值变得越来越大，永远不可能找到一个好的答案。</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-4.PNG" alt="学习率过小"></p>
<center>学习率过小</center>

<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-5.PNG" alt="img"></p>
<center>学习率过大</center>

<p><strong>两个主要挑战</strong></p>
<ol>
<li>收敛到局部最小值</li>
</ol>
<p>当使用梯度下降的时候，应该确保所有的特征有着相近的尺度范围（例如：使用 Scikit Learn 的 <code>StandardScaler</code>类），否则它将需要很长的时间才能够收敛。</p>
<h4 id="batch-GD"><a href="#batch-GD" class="headerlink" title="batch-GD"></a>batch-GD</h4><p>使用梯度下降的过程中，需要计算每一个 $\theta_j$ 下损失函数的梯度，即计算关于$\theta_j$ 的损失函数的偏导数，记为： </p>
<script type="math/tex; mode=display">\frac{\partial}{\partial \theta_j} MSE(\theta)=\frac{2}{m}\sum_{i=1}^{m}({\theta^T\cdot x^{(i)}-y^{(i)})x_j^{}(i)}</script><p>写成矩阵的形式：</p>
<script type="math/tex; mode=display">\nabla_\theta MSE(\theta)=\left( \begin{array}{c} \frac{\partial}{\partial \theta_0} MSE(\theta)\\\frac{\partial}{\partial \theta_1} MSE(\theta)\\...\\\frac{\partial}{\partial \theta_n} MSE(\theta) \end{array} \right)=\frac {2}{m} X^T\cdot (X\cdot \theta-y)</script><blockquote>
<p>在这个方程中每一步计算时都包含了整个训练集$X$，这也是为什么这个算法称为<strong>批量梯度下降</strong>：每一次训练过程都使用所有的的训练数据。因此，在大数据集上，其会变得相当的慢。然而，梯度下降的运算规模和特征的数量成正比。训练一个数千数量特征的线性回归模型使用梯度下降要比使用正态方程快的多。</p>
</blockquote>
<p>梯度下降的步长：</p>
<script type="math/tex; mode=display">\theta^{(next step)}=\theta-\eta\nabla_\theta MSE(\theta)</script><p>$\eta$ 为学习率</p>
<p>为了找到一个好的学习率，可以使用网格搜索，设置一个<strong>迭代次数</strong>。设置一个非常大的迭代次数，但是当梯度向量变得非常小（容差 $\epsilon$ ）的时候，结束迭代。</p>
<blockquote>
<p>收敛速率：</p>
<p>当损失函数是凸函数，且斜率不能突变（就像均方差损失函数那样），则它的批量梯度下降算法固定学习率之后，收敛速率是 $O(\frac{1}{iterations})$。换句话说，如果将容差 $\epsilon$ 缩小 10 倍（这样可以得到一个更精确的结果），算法的迭代次数大约会变成原来的 10 倍。</p>
</blockquote>
<h4 id="stochastic-gradient-descent"><a href="#stochastic-gradient-descent" class="headerlink" title="stochastic gradient descent"></a>stochastic gradient descent</h4><p>批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。</p>
<p>而随机梯度下降，每一步的梯度计算只随机选取训练集中的一个样本。由于每一次迭代，只需要在内存中有一个实例，算法非常快，这使随机梯度算法可以在大规模训练集上使用。</p>
<p>另一方面，由于其随机性，SGD呈现出更多的不规律性：到达最小值不是平缓的下降，损失函数会忽高忽低，只在大体上呈下降趋势。随着时间的推移，非常的靠近最小值，但不会停止在一个值上，而是一直在这个值附近摆动。因此，算法停止时，最后的参数不是最优值。</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-9.PNG" alt="img"></p>
<p>当损失函数很不规则时，随机梯度下降算法能够跳过局部最小值。因此，随机梯度下降在寻找全局最小值上比批量梯度下降表现要好。</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-6.PNG" alt="img"></p>
<p>虽然随机性可以很好的跳过局部最优值，但同时它却不能达到最小值。解决这个难题的一个办法是<strong>逐渐降低学习率</strong>。</p>
<p>开始时，走的每一步较大（有助于快速前进同时跳过局部最小值），然后变得越来越小，从而使算法到达全局最小值。，这个过程被称为<strong>模拟退火</strong>。</p>
<p>决定每次迭代的学习率的函数称为<code>learning schedule</code>。 如果学习速度降低得过快，可能会陷入局部最小值，甚至在到达最小值的半路就停止了。 如果学习速度降低得太慢，可能在最小值的附近长时间摆动，同时如果过早停止训练，最终只会出现次优解。</p>
<h4 id="mini-batch-GD"><a href="#mini-batch-GD" class="headerlink" title="mini-batch GD"></a>mini-batch GD</h4><p>在迭代的每一步，小批量梯度下降使用一个随机的小型实例集。与随机梯度相比主要优点在于可以通过矩阵运算的硬件优化得到一个较好的训练表现。</p>
<p>小批量梯度下降在参数空间上的表现比随机梯度下降要好的多，尤其在有大量的小型实例集时。作为结果，小批量梯度下降会比随机梯度更靠近最小值。但是，另一方面，它有可能陷在局部最小值中（在遇到局部最小值问题的情况下，和我们之前看到的线性回归不一样）。 </p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-11.PNG" alt="img"></p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E8%A1%A84-1.PNG" alt="img"></p>
<h4 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h4><p>对每个特征进行加权后作为新的特征，然后训练一个线性模型在这个扩展的特征集。 这种方法称为多项式回归。</p>
<p><code>from sklearn.preprocessing import PolynomialFeatures</code></p>
<p>当存在多个特征时，多项式回归能够找出特征之间的关系（这是普通线性回归模型无法做到的）。 这是因为<code>LinearRegression</code>会自动添加当前阶数下特征的所有组合。例如，如果有两个特征 $a,b$，使用 3 阶（<code>degree=3</code>）的<code>LinearRegression</code>时，不仅有 $a^3, a^2, b^3, b^2$，同时也会有它们的其他组合项 $ab, ab^2, a^2b$</p>
<p><code>PolynomialFeatures(degree=d)</code>把一个包含$n$个特征的数组转换为一个包含 $\frac{(n+d)!}{d!n!}$特征的数组。小心大量特征的组合爆炸！</p>
<h4 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h4><p><strong>如何估计一个模型的泛化能力</strong> </p>
<ol>
<li>使用交叉验证。若模型在训练集上表现良好，通过交叉验证指标却得出其泛化能力很差，模型过拟合。如果在这两方面都表现不好，那么欠拟合。这种方法可以告诉我们模型是太复杂还是太简单了。</li>
<li>观察学习曲线：画出模型在训练集上的表现，同时画出以训练集规模为自变量的训练集函数。为了得到图像，需要在训练集的不同规模子集上进行多次训练。</li>
</ol>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-15.PNG" alt="img"></p>
<center>欠拟合</center>

<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-16.PNG" alt="img"></p>
<center>过拟合</center>

<blockquote>
<p>偏差和方差的权衡</p>
<p>在统计和机器学习领域有个重要的理论：一个模型的<strong>泛化误差</strong>由三个不同误差的和决定：</p>
<ul>
<li>偏差：泛化误差的这部分误差是由于错误的假设决定的。例如实际是一个二次模型，你却假设了一个线性模型。一个高偏差的模型最容易出现欠拟合。</li>
<li>方差：这部分误差是由于模型对训练数据的微小变化较为敏感，一个多自由度的模型更容易有高的方差（例如一个高阶多项式模型），因此会导致模型过拟合。</li>
<li>不可约误差：这部分误差是由于数据本身的噪声决定的。降低这部分误差的唯一方法就是进行数据清洗（例如：修复数据源，修复坏的传感器，识别和剔除异常值）。</li>
</ul>
</blockquote>
<h4 id="线性模型的正则化"><a href="#线性模型的正则化" class="headerlink" title="线性模型的正则化"></a>线性模型的正则化</h4><p>正则化这个模型（即限制它）：模型有越少的自由度，就越难以拟合数据。例如，正则化一个多项式模型，一个简单的方法就是减少多项式的阶数。</p>
<p>对于一个线性模型，正则化的典型实现就是约束模型中参数的权重。</p>
<h5 id="岭回归（Ridge）"><a href="#岭回归（Ridge）" class="headerlink" title="岭回归（Ridge）"></a>岭回归（Ridge）</h5><p>岭回归（也称为 Tikhonov 正则化）是线性回归的正则化版：在损失函数上直接加上一个正则项。这使得学习算法不仅能够拟合数据，而且能够使模型的参数权重尽量的小。注意到这个正则项只有在训练过程中才会被加到损失函数。当得到完成训练的模型后，我们应该使用没有正则化的测量方法去评价模型的表现。</p>
<blockquote>
<p>提示</p>
<p>一般情况下，训练过程使用的损失函数和测试过程使用的评价函数是不一样的。除了正则化，还有一个不同：训练时的损失函数应该在优化过程中易于求导，而在测试过程中，评价函数更应该接近最后的客观表现。一个好的例子：在分类训练中我们使用对数损失（马上我们会讨论它）作为损失函数，但是我们却使用精确率/召回率来作为它的评价函数。</p>
</blockquote>
<p>岭回归损失函数：</p>
<p>$J(\theta)=MSE(\theta)+\alpha \frac{1}{2} \sum_{i=1}^{n}{\theta _i ^2}$</p>
<p>$\theta_0$是没有被正则化的，所以累加从i=1开始，而不是i=0开始。如果定义w作为特征的权重向量（$\theta_1$到$\theta_n$），则正则项可以简写成$\frac{1}{2}(||w||_2)^2$ ，$||\cdot||_2$表示$l_2$范数。</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-17.PNG" alt="img"></p>
<h6 id="封闭方程的解"><a href="#封闭方程的解" class="headerlink" title="封闭方程的解"></a>封闭方程的解</h6><script type="math/tex; mode=display">\hat \theta=(X^T\cdot X+\alpha A)^{-1}\cdot X^T\cdot y</script><p>A是一个除了左上角有一个0的nxn的单位矩阵，表示偏差$\theta_0​$不被正则化。</p>
<h6 id="随机梯度下降法求解"><a href="#随机梯度下降法求解" class="headerlink" title="随机梯度下降法求解"></a>随机梯度下降法求解</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sgd_reg = SGDRegressor(penalty=<span class="string">"l2"</span>)</span><br><span class="line"><span class="comment"># penalty参数指的是正则项的惩罚类型。指定“l2”表明你要在损失函数上添加一项：权重向量 l2 范数平方的一半，这就是简单的岭回归。</span></span><br></pre></td></tr></table></figure>
<h5 id="Lasso回归"><a href="#Lasso回归" class="headerlink" title="Lasso回归"></a>Lasso回归</h5><p>Lasso 回归（也称 Least Absolute Shrinkage，或者 Selection Operator Regression）是另一种正则化版的线性回归：就像岭回归那样，它也在损失函数上添加了一个正则化项，但是它使用权重向量的$l_1$ 范数而不是权重向量 $l_2$范数平方的一半。</p>
<p>$J(\theta)=MSE(\theta)+\alpha \sum_{i=1}^{n}{|\theta _i|}$</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-18.PNG" alt="img"></p>
<p>Lasso 回归的一个重要特征是它倾向于完全消除最不重要的特征的权重（即将它们设置为零）。例如，右图中的虚线所示$\alpha=10^{-7}$，曲线看起来像一条二次曲线，而且几乎是线性的，这是因为所有的高阶多项特征都被设置为零。换句话说，Lasso回归自动的进行特征选择同时输出一个稀疏模型（即，具有很少的非零权重）。</p>
<p>Lasso 回归子梯度向量:</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-93eea6b5c197bbc8d7be8b4c14e9f8f3.gif" alt="g(\theta,J)=\nabla_{\theta}MSE(\theta)+ \alpha{\left(\begin{matrix} sign(\theta_1)\\ sign(\theta_2)\\ \vdots \\ sign(\theta_n)\\ \end{matrix}\right)}\ where\ sign(\theta_i)= \begin{cases} -1, &amp;\theta_i&lt;0 \\ 0, &amp;\theta_i=0 \\ +1,&amp;\theta_i&gt;0 \\ \end{cases}"></p>
<h5 id="弹性网络（ElasticNet）"><a href="#弹性网络（ElasticNet）" class="headerlink" title="弹性网络（ElasticNet）"></a>弹性网络（ElasticNet）</h5><p>弹性网络介于 Ridge 回归和 Lasso 回归之间。它的正则项是 Ridge 回归和 Lasso 回归正则项的简单混合，同时你可以控制它们的混合率 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-4b43b0aee35624cd95b910189b3dc231.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-4b43b0aee35624cd95b910189b3dc231.gif" alt="r"></a></p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-e4da079f692fe35778bbdf1fdf120d99.gif" alt="J(\theta)=MSE(\theta)+r\alpha\sum\limits_{i=1}n\left|\theta_i \right|+\frac{1-r}{2}\alpha\sum\limits_{i=1}n\theta_i^2"></p>
<h5 id="早起停止法（Early-Stopping）"><a href="#早起停止法（Early-Stopping）" class="headerlink" title="早起停止法（Early Stopping）"></a>早起停止法（Early Stopping）</h5><p>对于迭代学习算法，有一种非常特殊的正则化方法，就像梯度下降在验证错误达到最小值时立即停止训练那样。我们称为早期停止法。</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-20.PNG" alt="img"></p>
<h5 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h5><blockquote>
<p>一些回归算法也可以用于分类（反之亦然）。 Logistic 回归（也称为 Logit 回归）通常用于估计一个实例属于某个特定类别的概率。 若估计的概率大于 50%，则模型预测这个实例属于当前类（称为正类，标记为“1”），反之预测它不属于当前类（即属于负类 ，标记为“0”）。 这样便成为了一个二元分类器。</p>
</blockquote>
<p>逻辑回归模型的概率估计：</p>
<p>$\hat p=h_\theta=\sigma(\theta^T\cdot x)$</p>
<p>其中$\sigma()$是Logistic函数，是一个sigmoid函数，输出是介于0到1之间的数字。</p>
<p>逻辑函数：</p>
<p>$\sigma(t)=\frac{1}{1+\exp(-t)}$</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-21.PNG" alt="img"></p>
<p>逻辑回归预测模型：</p>
<p>$\hat y = \left\{ \begin{array}{cl} 0,&amp; \hat p &lt; 0.5 \\ 1,&amp; \hat p \geq 0.5 \end{array} \right.$</p>
<h6 id="训练和损失函数"><a href="#训练和损失函数" class="headerlink" title="训练和损失函数"></a>训练和损失函数</h6><p>单个样本的损失函数：</p>
<p>$c(\theta)=\left \{ \begin{array}{lc} -\log(\hat p), &amp;y=1 \\ -\log(1-\hat p),&amp; y=0 \end{array}\right.$</p>
<p>整个训练集的损失函数只是所有训练实例的平均值。</p>
<p>逻辑回归的损失函数：<br>$J (\theta) = -\frac{1}{m} \sum_{i=1}^{m}[y^{(i)}log(\hat p^{(i)})+(1-y^{(i)})log(1-\hat p^{(i)})]$</p>
<p>但是这个损失函数对于求解最小化损失函数的$\theta是没有公式解的（没有等价的正态方程），但该损失函数是凸的，所以梯度下降（或任何其他优化算法）一定能够找到全局最小值。</p>
<p>逻辑回归损失函数的偏导数：</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial \theta_j}J(\theta_J)=\frac{1}{m}\sum_{i=1}^{m}(\sigma({\theta^T\cdot x^{(i)})-y^{(i)})x_j^{}(i)}</script><h6 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h6><p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_4/%E5%9B%BE4-23.PNG" alt="img"></p>
<p>概率=0.5的分界线</p>
<p>逻辑回归模型也可以 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-8524eb1789cf2093cfccc4c297138c7f.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-8524eb1789cf2093cfccc4c297138c7f.gif" alt="\ell_1"></a> 或者 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-f2d02eaf32cb7a351989198531c0d12a.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-f2d02eaf32cb7a351989198531c0d12a.gif" alt="\ell_2"></a> 惩罚使用进行正则化。Scikit-Learn 默认添加了 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-f2d02eaf32cb7a351989198531c0d12a.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-f2d02eaf32cb7a351989198531c0d12a.gif" alt="\ell_2"></a> 惩罚。</p>
<h5 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h5><p>Logistic 回归模型可以直接推广到支持多类别分类，不必组合和训练多个二分类器， 其称为 Softmax 回归或多类别 Logistic 回归。</p>
<p>当给定一个实例 $x$ 时，Softmax 回归模型首先计算 $k$ 类的分数，然$s_k(x)$ 后将分数应用在<code>Softmax</code>函数（也称为归一化指数）上，估计出每类的概率。</p>
<p>k类的Softmax得分：</p>
<p>$s_k(x)=\theta^T\cdot x$</p>
<blockquote>
<p>每个类都有自己独一无二的参数向量 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-9f888eddb683fe5f80f87f44bd727b08.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-9f888eddb683fe5f80f87f44bd727b08.gif" alt="\theta_k"></a>。 所有这些向量通常作为行放在参数矩阵 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-b9dce96eb3d5a71b28f9f198c28d2d1b.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-b9dce96eb3d5a71b28f9f198c28d2d1b.gif" alt="\Theta"></a> 中。</p>
</blockquote>
<p>Softmax函数：</p>
<script type="math/tex; mode=display">
\hat p_k=\sigma(s(x))_k=\frac{\exp(s_k(x))}{\sum_{j=1}^{K}{\exp(s_j(x))}}</script><p>$K$ 表示类的总数</p>
<p>Softmax 回归模型分类器预测结果:</p>
<p>$\hat y=argmax\  \sigma(s(x))_k=argmax\ s_k(x) = argmax \ (\theta_k^T\cdot x)$</p>
<blockquote>
<p>Softmax 回归分类器一次只能预测一个类（即它是多类的，但不是多输出的），因此它只能用于判断互斥的类别，如不同类型的植物。 不能用它来识别一张照片中的多个人。</p>
</blockquote>
<h6 id="训练和损失函数-1"><a href="#训练和损失函数-1" class="headerlink" title="训练和损失函数"></a>训练和损失函数</h6><p>目标是建立一个模型在目标类别上有着较高的概率（因此其他类别的概率较低）</p>
<p>交叉熵:</p>
<script type="math/tex; mode=display">
J (\theta) = -\frac{1}{m} \sum_{i=1}^{m}\sum_{k=1}^{K}y^{(i)}_klog(\hat p^{(i)}_k)</script><blockquote>
<p>交叉熵源于信息论。</p>
<p>假设你想要高效地传输每天的天气信息。如果有八个选项（晴天，雨天等），则可以使用3位对每个选项进行编码，因为 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-5f344a952e29992de54b8cfe645b2d5b.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-5f344a952e29992de54b8cfe645b2d5b.gif" alt="2^3=8"></a>。但是，如果你认为几乎每天都是晴天，更高效的编码“晴天”的方式是：只用一位（0）。剩下的七项使用四位（从 1 开始）。交叉熵度量每个选项实际发送的平均比特数。 如果你对天气的假设是完美的，交叉熵就等于天气本身的熵（即其内部的不确定性）。 但是，如果你的假设是错误的（例如，如果经常下雨）交叉熵将会更大，称为 Kullback-Leibler 散度（KL 散度）。</p>
<p>两个概率分布 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-83878c91171338902e0fe0fb97a8c47a.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-83878c91171338902e0fe0fb97a8c47a.gif" alt="p"></a> 和 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-7694f4a66316e53c8cdd9d9954bd611d.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-7694f4a66316e53c8cdd9d9954bd611d.gif" alt="q"></a> 之间的交叉熵定义为：<a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-6bc68f603b52e51645b4bbd318f8cdfe.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-6bc68f603b52e51645b4bbd318f8cdfe.gif" alt="H(p,q)=-\sum_xp(x)\log q(x)"></a>（分布至少是离散的）</p>
</blockquote>
<p>$k$ 类交叉熵的梯度向量:</p>
<p>$\nabla_{\theta_k} J (\Theta) = -\frac{1}{m} \sum_{i=1}^{m}(p^{(i)}_k - y^{(i)}_k )x^{(i)})$</p>
<h4 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h4><ol>
<li><p>如果你有一个数百万特征的训练集，你应该选择哪种线性回归训练算法？</p>
<blockquote>
<p>mini-batch GD或者SGD</p>
</blockquote>
</li>
<li><p>假设你训练集中特征的数值尺度（scale）有着非常大的差异，哪种算法会受到影响？有多大的影响？对于这些影响你可以做什么？</p>
<blockquote>
<p>如果训练集的特征尺度差距太大，损失函数的等高线会呈椭圆状，利用梯度下降来求最优解的过程中会很难收敛，梯度方向变化很剧烈，收敛速度很慢，并且可能不会收敛到最优点。</p>
<p>对数据进行缩放（StandardScaler），归一化</p>
</blockquote>
</li>
<li><p>训练 Logistic 回归模型时，梯度下降是否会陷入局部最低点？</p>
<blockquote>
<p>不会，其损失函数是一个凸函数。</p>
</blockquote>
</li>
<li><p>在有足够的训练时间下，是否所有的梯度下降都会得到相同的模型参数？</p>
<blockquote>
<p>不是，SGD因为随机性，达到损失函数只会呈现大体下降趋势，随着时间的推移，会非常靠近最小值，但只会在附近摆动。</p>
<p>mini-batch GD也会有一定的摆动。</p>
</blockquote>
</li>
<li><p>假设你使用批量梯度下降法，画出每一代的验证误差。当你发现验证误差一直增大，接下来会发生什么？你怎么解决这个问题？</p>
<blockquote>
<p>模型发散，难以收敛。减小学习率</p>
<p>或者模型过拟合，需要早停</p>
</blockquote>
</li>
<li><p>当验证误差升高时，立即停止小批量梯度下降是否是一个好主意？</p>
<blockquote>
<p>不是，因为mini-batch的损失函数随时间大体下降，但会有一定波动。</p>
<p>更好的选择是定期保存模型，如果经过很长一段时间仍然没有改进的话，可以恢复到保存的最好模型。</p>
</blockquote>
</li>
<li><p>哪个梯度下降算法（在我们讨论的那些算法中）可以最快到达解的附近？哪个的确实会收敛？怎么使其他算法也收敛？</p>
<blockquote>
<p>SGD最快</p>
<p>梯度下降法</p>
<p>逐步减小学习率</p>
</blockquote>
</li>
<li><p>假设你使用多项式回归，画出学习曲线，在图上发现学习误差和验证误差之间有着很大的间隙。这表示发生了什么？有哪三种方法可以解决这个问题？</p>
<blockquote>
<p>如果学习误差比验证误差小很多，说明发生了过拟合。</p>
<p>可以通过正则化（岭回归，Lasso回归和弹性网络）这个模型降低过拟合程度。</p>
<p>也可以提供更多的训练数据。</p>
<p>或者改变模型。</p>
</blockquote>
</li>
<li><p>假设你使用岭回归，并发现训练误差和验证误差都很高，并且几乎相等。你的模型表现是高偏差还是高方差？这时你应该增大正则化参数 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-7b7f9dbfea05c83784f8b85149852f08.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-7b7f9dbfea05c83784f8b85149852f08.gif" alt="\alpha"></a>，还是降低它？</p>
<blockquote>
<p>高偏差，应该减小$\alpha$。</p>
</blockquote>
</li>
<li><p>你为什么要这样做：</p>
</li>
</ol>
<ul>
<li><p>使用岭回归代替线性回归？</p>
<blockquote>
<p>模型有正则化的时候会比没有正则化的时候又更好的泛化性能，岭回归就是对模型的参数做了正则化，约束其幅值变化不能太大，否则会容易出现过拟合。</p>
</blockquote>
</li>
<li><p>Lasso 回归代替岭回归？</p>
<blockquote>
<p>当特征仅有少数是真正有用的时候，对重要的特征进行选择，将无用特征的权重降为零，增加模型的可解释性。</p>
</blockquote>
</li>
<li><p>弹性网络代替 Lasso 回归？</p>
<blockquote>
<p>当特征数量比样本的数量大的时候，或者特征之间有很强的相关性时，Lasso 可能会表现的不规律。</p>
</blockquote>
</li>
</ul>
<ol>
<li><p>假设你想判断一副图片是室内还是室外，白天还是晚上。你应该选择二个逻辑回归分类器，还是一个 Softmax 分类器？</p>
<blockquote>
<p>两个逻辑回归分类器</p>
</blockquote>
</li>
<li><p>在 Softmax 回归上应用批量梯度下降的早期停止法（不使用 Scikit-Learn）。</p>
<blockquote>
<p>​</p>
</blockquote>
</li>
</ol>
<hr>
<h2 id="第05章-支持向量机"><a href="#第05章-支持向量机" class="headerlink" title="第05章 支持向量机"></a>第05章 支持向量机</h2><p>能够做线性或者非线性的分类，回归，甚至异常值检测。SVM 特别适合应用于复杂但中小规模数据集的分类问题。</p>
<h4 id="线性支持向量机分类"><a href="#线性支持向量机分类" class="headerlink" title="线性支持向量机分类"></a>线性支持向量机分类</h4><p>SVM 分类器的判定边界实线，不仅分开了两种类别，而且还尽可能地远离了最靠近的训练数据点。（最大间隔分类）</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/5-1.jpg" alt="img"></p>
<p>判定边界是由位于“街道”边缘的样本点确定的，这些样本点被称为“支持向量”</p>
<h5 id="软间隔分类"><a href="#软间隔分类" class="headerlink" title="软间隔分类"></a>软间隔分类</h5><p>硬间隔分类有两个问题，</p>
<ol>
<li>只对线性可分的数据起作用.</li>
<li>对异常点敏感。</li>
</ol>
<p>在 Scikit-Learn 库的 SVM 类，可以用<code>C</code>超参数（惩罚系数）来控制这种平衡：较小的<code>C</code>会导致更宽的“街道”，但更多的间隔违规。</p>
<blockquote>
<p>SVM 模型过拟合，可以尝试通过减小超参数<code>C</code>去调整</p>
</blockquote>
<h4 id="非线性支持向量机分类"><a href="#非线性支持向量机分类" class="headerlink" title="非线性支持向量机分类"></a>非线性支持向量机分类</h4><h5 id="增加更多的特征"><a href="#增加更多的特征" class="headerlink" title="增加更多的特征"></a>增加更多的特征</h5><p>通过 Scikit-Learn，可以创建一个流水线（Pipeline）去包含多项式特征（PolynomialFeatures）变换，然后StandardScaler.</p>
<h5 id="多项式核"><a href="#多项式核" class="headerlink" title="多项式核"></a>多项式核</h5><p>添加多项式特征会：大量特征导致的组合爆炸</p>
<h5 id="增加相似特征"><a href="#增加相似特征" class="headerlink" title="增加相似特征"></a>增加相似特征</h5><p>使用相似函数（similarity funtion）计算每个样本与特定地标（landmark）的相似度。</p>
<p>定义一个相似函数，即<strong>高斯径向基函数（Gaussian Radial Basis Function，RBF）</strong>，设置<code>γ = 0.3</code></p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-8b908429a5b5ee2e519f8caa16f82ee1.gif" alt="\phi_{\gamma}(x, \ell) = exp(-\gamma \|x - \ell \|^2)"></p>
<p>是个从 0 到 1 的钟型函数。</p>
<h5 id="高斯-RBF-核"><a href="#高斯-RBF-核" class="headerlink" title="高斯 RBF 核"></a>高斯 RBF 核</h5><p>相似特征法在所有额外特征上的计算成本可能很高，特别是在大规模的训练集上。</p>
<h5 id="计算复杂性"><a href="#计算复杂性" class="headerlink" title="计算复杂性"></a>计算复杂性</h5><p><code>LinearSVC</code>类基于<code>liblinear</code>库，它实现了线性 SVM 的优化算法。它并不支持核技巧，但是它样本和特征的数量几乎是线性的：训练时间复杂度大约为<code>O(m × n)</code>。</p>
<p>SVC 类基于<code>libsvm</code>库，它实现了支持核技巧的算法。训练时间复杂度通常介<code>于O(m^2 × n)</code>和<code>O(m^3 × n)</code>之间。</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/tb-5-1.jpg" alt="img"></p>
<h4 id="SVM回归"><a href="#SVM回归" class="headerlink" title="SVM回归"></a>SVM回归</h4><p>SVM 回归任务是限制间隔违规情况下，尽量放置更多的样本在“街道”上。“街道”的宽度由超参数<code>ϵ</code>控制。</p>
<p>可以使用 Scikit-Learn 的<code>LinearSVR</code>类去实现线性 SVM 回归。</p>
<p>处理非线性回归任务，可以使用核化的 SVM 模型。</p>
<blockquote>
<p>LinearSVR没有support_属性</p>
</blockquote>
<h4 id="背后机制"><a href="#背后机制" class="headerlink" title="背后机制"></a>背后机制</h4><p>首先，关于符号的约定：在第 4 章，我们将所有模型参数放在一个矢量<code>θ</code>里，包括偏置项<code>θ0</code>，<code>θ1</code>到<code>θn</code>的输入特征权重，和增加一个偏差输入<code>x0 = 1</code>到所有样本。</p>
<p>在本章中，我们将使用一个不同的符号约定，在处理 SVM 上，这更方便，也更常见：<strong>偏置项被命名为b，特征权重向量被称为w</strong>，在输入特征向量中不再添加偏置特征。</p>
<h5 id="决策函数和预测"><a href="#决策函数和预测" class="headerlink" title="决策函数和预测"></a>决策函数和预测</h5><p>线性 SVM 分类器通过简单地计算决策函数 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-7781983bd977537b3c5d060e217ea82a.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-7781983bd977537b3c5d060e217ea82a.gif" alt="w \cdot x+b = w_1 x_1 + ... + w_n x_n + b"></a> 来预测新样本的类别：如果结果是正的，预测类别<code>ŷ</code>是正类，为 1，否则他就是负类，为 0。</p>
<p>$\hat y = \left \{ \begin{array}{ll}  0 &amp; if \ w^T\cdot x+b&lt;0 \\1 &amp; \ w^T\cdot x+b\geq0\end{array}\right.$</p>
<p>训练线性 SVM 分类器意味着找到<code>w</code>值和<code>b</code>值使得这一个间隔尽可能大，同时避免间隔违规（硬间隔）或限制它们（软间隔）</p>
<h5 id="训练目标"><a href="#训练目标" class="headerlink" title="训练目标"></a>训练目标</h5><p>决策函数的斜率：它等于权重向量的范数 $||\omega||$ ，斜率除于 2，那么间隔将增加两倍。权重向量<code>w</code>越小，间隔越大。</p>
<p>目标是最小化$||\omega||$ ，从而获得大的间隔。如果想要避免间隔违规（硬间隔），对于正的训练样本，需要决策函数大于 1，对于负训练样本，小于 -1。即：$t^{(i)}\ w^T\cdot x^{(i)}+b\geq1$</p>
<p><strong>可以将硬间隔线性 SVM 分类器表示为约束优化问题:</strong></p>
<script type="math/tex; mode=display">
minimize \frac{1}{2} \omega^T\cdot \omega \\
subject \  to\  t^{(i)}\ w^T\cdot x^{(i)}+b\geq1 \ \ \ for \ i=1,2,3,\cdots , m</script><blockquote>
<p><a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-9a84ebda628c391e3046dfc2307e3c85.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-9a84ebda628c391e3046dfc2307e3c85.gif" alt="1/2 w^T w"></a> 等于 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-98c822c91ab5af02c383eb03fa5b5446.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-98c822c91ab5af02c383eb03fa5b5446.gif" alt="1/2 \|w\|^2"></a>，最小化 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-9a84ebda628c391e3046dfc2307e3c85.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-9a84ebda628c391e3046dfc2307e3c85.gif" alt="1/2 w^T w"></a>，而不是最小化 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-03549015bd48d379883d926e6857b448.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-03549015bd48d379883d926e6857b448.gif" alt="\|w\|"></a>。因为<a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-03549015bd48d379883d926e6857b448.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-03549015bd48d379883d926e6857b448.gif" alt="\|w\|"></a> 在<code>w=0</code>处是不可微的。优化算法在可微函数表现得更好。</p>
</blockquote>
<p>软间隔：</p>
<p>对每个样本应用一个松弛变量（slack variable）$\zeta^{(i)}\geq0$ 。$\zeta^{(i)}$表示了第<code>i</code>个样本允许违规间隔的程度。</p>
<p>现在有两个不一致的目标：</p>
<ol>
<li>使松弛变量尽可能的小，从而减小间隔违规.</li>
<li>使<code>1/2 w·w</code>尽量小，从而增大间隔。</li>
</ol>
<p>这时<code>C</code>超参数发挥作用：它允许我们在两个目标之间权衡。</p>
<script type="math/tex; mode=display">
minimize\  \frac{1}{2} \omega^T\cdot \omega +C\sum_{i=1}^{m}\zeta^{(i)}\\

subject \  to\  t^{(i)}\ w^T\cdot x^{(i)}+b\geq1-\zeta^{(i)}\ \  and \ \ \zeta^{(i)} \geq0 \ \  for \ i=1,2,3,\cdots , m</script><h5 id="二次间隔"><a href="#二次间隔" class="headerlink" title="二次间隔"></a>二次间隔</h5><p>硬间隔和软间隔都是<strong>线性约束的凸二次规划优化问题</strong>。这些问题被称之为二次规划（QP）问题。</p>
<h5 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h5><p>给出一个约束优化问题，即原始问题（primal problem），它可能表示不同但是和另一个问题紧密相连，称为对偶问题（Dual Problem）。对偶问题的解通常是对原始问题的解给出一个下界约束，但在某些条件下，它们可以获得相同解。</p>
<p><strong>线性 SVM 的对偶形式（核技巧的基本）</strong>：</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-6.gif" alt="img"></p>
<p>一旦找到最小化公式的向量<code>α</code>，可以计算<code>w</code>和<code>b</code>，从而使原始问题最小化。</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-7.gif" alt="img"></p>
<p>训练样本的数量比特征数量小的时候，对偶问题比原始问题要快得多。</p>
<h5 id="核化支持向量机"><a href="#核化支持向量机" class="headerlink" title="核化支持向量机"></a>核化支持向量机</h5><p>把一个 2 次多项式变换应用到二维空间的训练集，然后在变换后的训练集上训练一个线性SVM分类器。</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-8.gif" alt="img"></p>
<p>转换后向量的点积等于原始向量点积的平方:</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-9.gif" alt="img"></p>
<p><strong>核技巧的精髓:</strong></p>
<p>不需要对训练样本进行转换：仅仅需要在对偶问题公式中，将点积替换成它点积的平方。</p>
<p>函数$K(a, b) = (a^T b)^2$ 被称为二次多项式核（polynomial kernel）。在机器学习，核函数是一个能计算点积的函数，并只基于原始向量<code>a</code>和<code>b</code>，不需要计算（甚至知道）转换<code>ϕ</code>。</p>
<p><strong>一些常见的核函数</strong></p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-10.gif" alt="img"></p>
<blockquote>
<p>Mercer 定理</p>
<p>根据 Mercer 定理，如果函数<code>K(a, b)</code>满足一些 Mercer 条件的数学条件(<code>K</code>函数在参数内必须是连续，对称，即<code>K(a, b)=K(b, a)</code>，等)，那么存在函数<code>ϕ</code>，将<code>a</code>和<code>b</code>映射到另一个空间（可能有更高的维度），有 $K(a, b) = \phi(a)^T ϕ(b)$。所以可以用<code>K</code>作为核函数，即使不知道<code>ϕ</code>。使用高斯核（Gaussian RBF kernel）情况下，它实际是将每个训练样本映射到无限维空间，所以不需要知道是怎么执行映射的也是一件好事。</p>
<p>注意一些常用核函数（例如 Sigmoid 核函数）并不满足所有的 Mercer 条件，然而在实践中通常表现得很好。</p>
</blockquote>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-11.gif" alt="img"></p>
<p>也需要使用同样的技巧来计算偏置项<code>b</code></p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-12.gif" alt="img"></p>
<p>支持向量才满足<code>α(i)≠0</code>，做出预测只涉及计算为支持向量部分的输入样本 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-992fd41f053d328db0ca0287eed0e2e9.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-992fd41f053d328db0ca0287eed0e2e9.gif" alt="x^{(n)}"></a> 的点积，而不是全部的训练样本。</p>
<h5 id="在线支持向量机"><a href="#在线支持向量机" class="headerlink" title="在线支持向量机"></a>在线支持向量机</h5><p>线学习意味着增量地学习，不断有新实例。</p>
<p>于线性SVM分类器，一种方式是使用梯度下降（例如使用<code>SGDClassifire</code>）最小化代价函数：</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-13.gif" alt="img"></p>
<p>第一个和会使模型有一个小的权重向量<code>w</code>，从而获得一个更大的间隔。第二个和计算所有间隔违规的总数。如果样本位于“街道”上和正确的一边，或它与“街道”正确一边的距离成比例，则间隔违规等于 0。最小化保证了模型的间隔违规尽可能小并且少。</p>
<blockquote>
<p>Hinge 损失</p>
<p>函数<code>max(0, 1–t)</code>被称为 Hinge 损失函数（如下）。当<code>t≥1</code>时，Hinge 值为 0。如果<code>t&lt;1</code>,它的导数（斜率）为 -1，若<code>t&gt;1</code>，则等于0。在<code>t=1</code>处，它是不可微的，但就像套索回归（Lasso Regression）一样，仍然可以在<code>t=0</code>时使用梯度下降法（即 -1 到 0 之间任何值）<img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/5-hinge.jpg" alt="img"></p>
</blockquote>
<p>大规模的非线性问题，可能需要考虑使用神经网络。</p>
<h4 id="Exercise-1"><a href="#Exercise-1" class="headerlink" title="Exercise"></a>Exercise</h4><ol>
<li><p>支持向量机背后的基本思想是什么</p>
<blockquote>
<p>支持向量机的背后的基本思想：寻找具有最大间隔的分类平面，来对正负样本进行分类。</p>
</blockquote>
</li>
<li><p>什么是支持向量</p>
<blockquote>
<p>支持向量就是最大间隔平面边界上样本点，这些样本点被称为“支持向量”</p>
</blockquote>
</li>
<li><p>当使用 SVM 时，为什么标准化输入很重要？</p>
<blockquote>
<p>因为SVM 对特征缩放比较敏感，对特征进行缩放可以使得判定边界更宽。</p>
<p>SVM寻找具有最大分类间隔的平面，如果不对输入进行归一化，SVM会忽略小的特征。</p>
</blockquote>
</li>
<li><p>分类一个样本时，SVM 分类器能够输出一个置信值吗？概率呢？</p>
<blockquote>
<p>SVM分类器通过简单地计算决策函数$\omega \cdot x+b$来预测新样本的类别，如果小于0，则为负类，如果大于等于0，则为正类。</p>
<p>SVM分类器不会输出每个类别的概率。</p>
</blockquote>
</li>
<li><p>在一个有数百万训练样本和数百特征的训练集上，你是否应该使用 SVM 原始形式或对偶形式来训练一个模型？</p>
<blockquote>
<p>线性SVM的训练时间复杂度约为O(m x n)，而对偶形式的复杂度通常介于O(m^2 x n) 到 O(m^3 x n)，m为实例个数，当训练样本变大时，它将变得极其慢。</p>
<p>所以对于大规模线性问题，可以使用SVM的原始形式，而对于大规模的非线性问题，可能需要考虑使用神经网络。</p>
</blockquote>
</li>
<li><p>假设你用 RBF 核来训练一个 SVM 分类器，如果对训练集欠拟合：你应该增大或者减小<code>γ</code>吗？调整参数<code>C</code>呢？</p>
<blockquote>
<p>RBF的形式：$\phi_{\gamma}(x, \ell) = exp(-\gamma |x - \ell |^2)$，是一个钟形曲线，<code>γ</code> 越大，钟形曲线越窄，每个样本的影响范围变得更小，边界更细化。</p>
<p>超参数<code>C</code>控制SCM分类模型的软硬，较小的<code>C</code>会导致更宽的分类间隔平面，但更多的间隔违规。</p>
<p>所以欠拟合时可以增大<code>γ</code>，增大<code>C</code></p>
</blockquote>
</li>
<li><p>使用现有的 QP 解决方案，你应该怎么样设置 QP 参数（<code>H</code>，<code>f</code>，<code>A</code>，和<code>b</code>）去解决一个软间隔线性 SVM 分类器问题？</p>
<blockquote>
<p>软间隔分类：</p>
<script type="math/tex; mode=display">
minimize\  \frac{1}{2} \omega^T\cdot \omega +C\sum_{i=1}^{m}\zeta^{(i)}\\

subject \  to\  t^{(i)}\ w^T\cdot x^{(i)}+b\geq1-\zeta^{(i)}\ \  and \ \ \zeta^{(i)} \geq0 \ \  for \ i=1,2,3,\cdots , m</script><p>QP(二次规划问题，即线性约束的凸二次规划优化问题)</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_5/eq-5-5.gif" alt="img"></p>
<p>H 是 $n_p \times n_p$的单位矩阵，左上角为0（忽略偏置向）</p>
<p>p = $（b,\ \omega_1\ \cdots\ \omega_n)^T$</p>
<p>$f^T\cdot p \ = \     C\cdot max(1-p^T\cdot A_i,0))$</p>
<p>$b_i=1-max(1-p^T\cdot A_i,0)$</p>
<p>$a_i^j=x_j^{(i)}\  \  i=1\cdots n_c;\ j=1\cdots n_p$ 第i个实例的第j个特征，但$x^{(i)}$带一个1的偏置项。</p>
</blockquote>
</li>
<li><p>在一个线性可分的数据集训练一个<code>LinearSVC</code>，并在同一个数据集上训练一个<code>SVC</code>和<code>SGDClassifier</code>，看它们是否产生了大致相同效果的模型。</p>
<blockquote>
<p>​</p>
</blockquote>
</li>
<li><p>在 MNIST 数据集上训练一个 SVM 分类器。因为 SVM 分类器是二元的分类，你需要使用一对多（one-versus-all）来对 10 个数字进行分类。你可能需要使用小的验证集来调整超参数，以加快进程。最后你能达到多少准确度？</p>
</li>
<li><p>在加利福尼亚住宅（California housing）数据集上训练一个 SVM 回归模型</p>
</li>
</ol>
<hr>
<h2 id="第06章-决策树"><a href="#第06章-决策树" class="headerlink" title="第06章 决策树"></a>第06章 决策树</h2><p>决策树是一种多功能机器学习算法， 即可以执行分类任务也可以执行回归任务， 甚至包括多输出（multioutput）任务。</p>
<h3 id="决策树的训练和可视化"><a href="#决策树的训练和可视化" class="headerlink" title="决策树的训练和可视化"></a>决策树的训练和可视化</h3><blockquote>
<p>决策树的众多特性之一就是， 它不需要太多的数据预处理， 尤其是不需要进行特征的缩放或者归一化。</p>
</blockquote>
<ul>
<li>节点的<code>samples</code>属性统计出它应用于多少个训练样本实例。</li>
<li>节点的<code>value</code>属性：这个节点对于每一个类别的样例有多少个。</li>
<li>节点的<code>Gini</code>属性用于测量它的纯度：如果一个节点包含的所有训练样例全都是同一类别的，这个节点是纯的（<code>Gini=0</code>）。</li>
</ul>
<p><strong>Gini分数$G_i$</strong></p>
<p>$G_i=1-\sum_{k=1}^{n}P_{i,k}^2$</p>
<p>$P_{i,k}$是第<code>i</code>个节点中训练实例为的<code>k</code>类实例的比例。</p>
<blockquote>
<p>Scikit-Learn 用的是 CART 算法， CART 算法仅产生二叉树：每一个非叶节点总是只有两个子节点（只有是或否两个结果）。然而，像 ID3 这样的算法可以产生超过两个子节点的决策树模型。</p>
</blockquote>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_6/102" alt="1528081141956"></p>
<h3 id="估计分类概率"><a href="#估计分类概率" class="headerlink" title="估计分类概率"></a>估计分类概率</h3><p>决策树还可以估计某个实例属于特定类<code>k</code>的概率：首先遍历树来查找此实例的叶节点，然后它返回此节点中类<code>k</code>的训练实例的比例。</p>
<blockquote>
<p>假设你发现了一个花瓣长 5 厘米，宽 1.5 厘米的花朵。相应的叶节点是深度为 2 的左节点，因此决策树应该输出以下概率：Iris-Setosa 为 0%（0/54），Iris-Versicolor 为 90.7%（49/54），Iris-Virginica 为 9.3%（5/54）。当然，如果你要求它预测具体的类，它应该输出 Iris-Versicolor（类别 1），因为它具有最高的概率。</p>
</blockquote>
<h3 id="CART训练算法"><a href="#CART训练算法" class="headerlink" title="CART训练算法"></a>CART训练算法</h3><p>Scikit-Learn 用<strong>分裂回归树（Classification And Regression Tree，简称 CART）算法训练决策树（也叫“增长树”）</strong>。</p>
<p>首先使用单个特征<code>k</code>和阈值 $t_k$ 例如，（“花瓣长度<code>≤2.45cm</code>”）将训练集分成两个子集。它如何选择<code>k</code>和 $t_k$ ？它寻找到能够产生最纯粹子集的一对 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-32cb6265eb19ce4be37ecf6650ff766a.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-32cb6265eb19ce4be37ecf6650ff766a.gif" alt="(k, t_k)"></a>，然后通过子集大小加权计算。</p>
<p>算法会尝试最小化成本函数：</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_6/104" alt="1528086977613"></p>
<blockquote>
<p>CART 算法是一种贪婪算法：贪婪地搜索最高级别的最佳分割方式，然后在每个深度重复该过程。 </p>
<p>不检查分割是否能够在几个级别中的全部分割可能中找到最佳方法。</p>
<p>贪婪算法通常会产生一个相当好的解决方法，但它不保证这是全局中的最佳解决方案。</p>
</blockquote>
<h4 id="NP完全问题"><a href="#NP完全问题" class="headerlink" title="NP完全问题"></a>NP完全问题</h4><p>寻找最佳的决策树是NP完成问题(<code>Non-deterministic Polynomial</code>，即多项式复杂程度的非确定性问题)，也会简称为NP-C问题。</p>
<p>决策树的这一特点，说明无法利用计算机在多项式时间内，找出全局最优的解。也正因为如此，大多数决策树算法都采用启发式的算法，如&amp;<strong>贪心算法</strong>，来指导对假设空间的搜索。可以说，决策树最后的结果，是在每一步、每一个节点上做的局部最优选择。决策树得到的结果，无法保证是全局最优解。</p>
<h5 id="P类问题"><a href="#P类问题" class="headerlink" title="P类问题"></a>P类问题</h5><blockquote>
<p> 所有可以在多项式时间内求解的判定问题构成<strong>P类问题</strong></p>
</blockquote>
<p><strong>判定问题</strong>是指回答结果输出为<code>Yes</code>或<code>No</code>的问题，比如：3233是否可以写成两个大于1的数字的乘积？</p>
<p>在设计程序时，需要评估这个程序的时间复杂度，即衡量当问题规模变大后，程序执行所需的时间增长会有多快。如$O(1)$表示常数级别，即不管问题的规模变大多少倍，所耗的时间不会改变；$O(N^2)$表示平方级别，即当问题规模增大至2倍时，所花费的时间则放大至4倍；$O(2^N)$表示指数级别，即当问题规模倍数扩大时，所用时间会呈指数放大。</p>
<p><strong>多项式时间</strong>则是指$O(1)、O(logN)、O(N^2)$等这类可用多项式表示的时间复杂度，通常认为计算机可解决的问题只限于多项式时间内。而$O(2^N)、O(N!)$这类非多项式级别的问题，其复杂度往往已经到了计算机都接受不了的程度。</p>
<h5 id="NP类问题"><a href="#NP类问题" class="headerlink" title="NP类问题"></a>NP类问题</h5><blockquote>
<p>所有非确定性多项式时间内可解的判定问题构成<strong>NP类问题</strong></p>
</blockquote>
<p>NP类问题将问题分为求解和验证两个阶段，问题的求解是非确定性的，无法在多项式时间内得到答案，而问题的验证却是确定的，能够在多项式时间里确定结果。</p>
<p>比如：是否存在一个公式可以计算下一个质数是多少？这个问题的答案目前是无法直接计算出来的，但是如果某人给出了一个公式，我们却可以在多项式时间里对这个公式进行验证。</p>
<h5 id="NP完全问题-1"><a href="#NP完全问题-1" class="headerlink" title="NP完全问题"></a>NP完全问题</h5><blockquote>
<p>NP类问题的一种特殊情况，这类问题中每个问题的复杂度与整个类的复杂度有关联性，假如其中任意一个问题在多项式时间内可解的，则这一类问题都是多项式时间可解。这些问题被称为<strong>NP完全问题</strong>。</p>
</blockquote>
<p>总结这几类问题的特点，可参考如下这个表格：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>问题类型</th>
<th>是否能在多项式时间内求解</th>
<th>是否能在多项式时间内验证</th>
</tr>
</thead>
<tbody>
<tr>
<td>P</td>
<td>是</td>
<td>是</td>
</tr>
<tr>
<td>NP</td>
<td>是 or 否</td>
<td>是</td>
</tr>
<tr>
<td>NP-C</td>
<td>未知</td>
<td>是</td>
</tr>
</tbody>
</table>
</div>
<p>参考：<a href="https://www.jianshu.com/p/dcb0b52f4935" target="_blank" rel="noopener">https://www.jianshu.com/p/dcb0b52f4935</a></p>
<h3 id="计算复杂度"><a href="#计算复杂度" class="headerlink" title="计算复杂度"></a>计算复杂度</h3><p>建立好决策树模型后， 做出<strong>预测</strong>需要遍历决策树， 从根节点一直到叶节点。决策树通常近似左右平衡，因此遍历决策树需要经历大致  $O(log_2m)$个节点。由于每个节点只需要检查一个特征的值，因此总体预测复杂度仅为 $O(log_2m)$，与特征的数量无关。</p>
<p><strong>训练</strong>算法的时候（训练和预测不同）需要比较所有特征（如果设置了<code>max_features</code>会更少一些）在每个节点的所有样本上。训练复杂度为$O(nmlog_m)$</p>
<h3 id="基尼不纯度和信息熵"><a href="#基尼不纯度和信息熵" class="headerlink" title="基尼不纯度和信息熵"></a>基尼不纯度和信息熵</h3><p>默认算法使用 Gini 不纯度来进行检测， 但是也可以通过将标准超参数设置为<code>&quot;entropy&quot;</code>来使用熵不纯度进行检测。</p>
<p><strong>熵的计算：</strong></p>
<script type="math/tex; mode=display">
H_i=-\sum_{k=1\ P_{i,k=!0}}^n{P_{i,k}}log(p_{i,k})</script><p>熵的减少通常称为<strong>信息增益</strong></p>
<blockquote>
<p>基尼指数计算稍微快一点，所以这是一个很好的默认值。</p>
<p>但是，也有的时候它们会产生不同的树，基尼指数会趋于在树的分支中将最多的类隔离出来，而熵指数趋向于产生略微平衡一些的决策树模型。</p>
</blockquote>
<h3 id="正则化超参数"><a href="#正则化超参数" class="headerlink" title="正则化超参数"></a>正则化超参数</h3><p>决策树几乎不对训练数据做任何假设（于此相反的是线性回归等模型，这类模型通常会假设数据是符合线性关系的）。这一类的模型通常会被称为<strong>非参数模型&amp;</strong>，这不是因为它没有任何参数（通常也有很多），而是因为<strong>在训练之前没有确定参数的具体数量，所以模型结构可以根据数据的特性自由生长。</strong></p>
<p>如果不添加约束，树结构模型通常将根据训练数据调整自己，使自身能够很好的拟合数据，而这种情况下大多数会导致<strong>模型过拟合</strong>。</p>
<blockquote>
<p><strong><code>DecisionTreeClassifier</code>类还有一些其他的参数用于限制树模型的形状:</strong></p>
<p><code>min_samples_split</code>（节点在被分裂之前必须具有的最小样本数）</p>
<p><code>min_samples_leaf</code>（叶节点必须具有的最小样本数），<code>min_weight_fraction_leaf</code>（和<code>min_samples_leaf</code>相同，但表示为加权总数的一小部分实例）</p>
<p><code>max_leaf_nodes</code>（叶节点的最大数量）</p>
<p><code>max_features</code>（在每个节点被评估是否分裂的时候，具有的最大特征数量）</p>
<p>增加<code>min_* hyperparameters</code>或者减少<code>max_* hyperparameters</code>会使模型正则化。</p>
<p><strong>剪枝</strong></p>
<p>在没有任何约束条件下训练决策树模型，让模型自由生长，然后再对不需要的节点进行剪枝。</p>
<p>当一个节点的全部子节点都是叶节点时，如果它对纯度的提升不具有统计学意义，我们就认为这个分支是不必要的。</p>
<p>标准的假设检验，例如卡方检测，通常会被用于评估一个概率值 — 即改进是否纯粹是偶然性的结果（也叫原假设）</p>
<p>如果 p 值比给定的阈值更高（通常设定为 5%，也就是 95% 置信度，通过超参数设置），那么节点就被认为是非必要的，它的子节点会被删除。</p>
<p>这种剪枝方式将会一直进行，直到所有的非必要节点都被删光。</p>
</blockquote>
<h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h3><p>Scikit-Learn 的<code>DecisionTreeRegressor</code>类构建回归树，不再以最小化不纯度的方式分割训练集，而是试图以最小化 MSE 的方式分割训练集。</p>
<blockquote>
<p> 每个区域的预测值总是该区域中实例的平均目标值。</p>
</blockquote>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_6/108" alt="1528375227547"></p>
<h3 id="不稳定性"><a href="#不稳定性" class="headerlink" title="不稳定性"></a>不稳定性</h3><p>决策树的特点：</p>
<ol>
<li><p>容易理解和解释，易于使用且功能丰富而强大。</p>
</li>
<li><p>决策树很喜欢设定正交化的决策边界，（所有边界都是和某一个轴相垂直的），这使得它对训练数据集的旋转很敏感。</p>
<blockquote>
<p>解决办法：主成分分析PCA</p>
</blockquote>
</li>
<li><p>决策时的主要问题是它对训练数据的微小变化非常敏感</p>
</li>
</ol>
<h3 id="exercise"><a href="#exercise" class="headerlink" title="exercise"></a>exercise</h3><ol>
<li><p>在 100 万例训练集上训练（没有限制）的决策树的近似深度是多少？</p>
<blockquote>
<p>$log_2(m)$，m=100万时，该值大约为20</p>
</blockquote>
</li>
<li><p>节点的基尼指数比起它的父节点是更高还是更低？它是通常情况下更高/更低，还是永远更高/更低？</p>
<blockquote>
<p>节点的基尼指数通常低于其父节点，这是由CART数的训练损失函数所确定的，每个节点经过最优属性分割之后，子节点的基尼指数之和都会小于父节点的基尼指数，然而如果一个孩子节点的基尼指数比另外一个的基尼指数小，则可能比其父节点的基尼不纯度更大，但是其基尼不纯度的增加肯定小于另外孩子基尼不纯度的减少。</p>
</blockquote>
</li>
<li><p>如果决策树过拟合了，减少最大深度是一个好的方法吗？</p>
<blockquote>
<p>是一个好方法，可以对模型进行约束，使其规范化</p>
</blockquote>
</li>
<li><p>如果决策树对训练集欠拟合了，尝试缩放输入特征是否是一个好主意？</p>
<blockquote>
<p>不是，因为决策树对于数据的缩放并不敏感</p>
</blockquote>
</li>
<li><p>如果对包含 100 万个实例的数据集训练决策树模型需要一个小时，在包含 1000 万个实例的培训集上训练另一个决策树大概需要多少时间呢？</p>
<blockquote>
<p>训练复杂度$O(n\times m \log(m))$，如果实例m增加10倍，则复杂度变为$10\times\log(10m)/\log(m)$ 倍，m=100万时，该值为11.7，所以需要11.7个小时</p>
</blockquote>
</li>
<li><p>如果你的训练集包含 100,000 个实例，设置<code>presort=True</code>会加快训练的速度吗？</p>
<blockquote>
<p>只有当数据集小于几千时，才会加速训练实例。如果它包含100,000个实例，那么设置presort=True的训练将会慢得多。</p>
</blockquote>
</li>
<li><p>对<code>moons</code>数据集进行决策树训练并优化模型。</p>
<ol>
<li><p>通过语句<code>make_moons(n_samples=10000, noise=0.4)</code>生成<code>moons</code>数据集</p>
</li>
<li><p>通过<code>train_test_split()</code>将数据集分割为训练集和测试集。</p>
</li>
<li><p>进行交叉验证，并使用网格搜索法寻找最好的超参数值（使用<code>GridSearchCV</code>类的帮助文档）</p>
<p>提示: 尝试各种各样的<code>max_leaf_nodes</code>值</p>
</li>
<li><p>使用这些超参数训练全部的训练集数据，并在测试集上测量模型的表现。你应该获得大约 85% 到 87% 的准确度。</p>
</li>
</ol>
</li>
<li><p>生成森林</p>
<ol>
<li>接着前边的练习，现在，让我们生成 1,000 个训练集的子集，每个子集包含 100 个随机选择的实例。提示：你可以使用 Scikit-Learn 的<code>ShuffleSplit</code>类。</li>
<li>使用上面找到的最佳超参数值，在每个子集上训练一个决策树。在测试集上测试这 1000 个决策树。由于它们是在较小的集合上进行了训练，因此这些决策树可能会比第一个决策树效果更差，只能达到约 80% 的准确度。</li>
<li>见证奇迹的时刻到了！对于每个测试集实例，生成 1,000 个决策树的预测结果，然后只保留出现次数最多的预测结果（您可以使用 SciPy 的<code>mode()</code>函数）。这个函数使你可以对测试集进行多数投票预测。</li>
<li>在测试集上评估这些预测结果，你应该获得了一个比第一个模型高一点的准确率，（大约 0.5% 到 1.5%），恭喜，你已经弄出了一个随机森林分类器模型!</li>
</ol>
</li>
</ol>
<h2 id="第07章-集成学习和随机森林"><a href="#第07章-集成学习和随机森林" class="headerlink" title="第07章 集成学习和随机森林"></a>第07章 集成学习和随机森林</h2><p>合并了一组分类器的预测（像分类或者回归），得到一个比单一分类器更好的预测结果。这一组分类器就叫做<strong>集成</strong>；因此，这个技术就叫做<strong>集成学习</strong>，一个集成学习算法就叫做<strong>集成方法</strong>。一种决策树的集成就叫做<strong>随机森林</strong>。</p>
<p>即使每一个分类器都是一个<strong>弱学习器</strong>（意味着它们也就比瞎猜好点），集成后仍然是一个<strong>强学习器</strong>（高准确率），只要有足够数量的弱学习者，他们就足够多样化。</p>
<h3 id="投票分类"><a href="#投票分类" class="headerlink" title="投票分类"></a>投票分类</h3><p>一个非常简单去创建一个更好的分类器的方法就是去整合每一个分类器的预测然后经过投票去预测分类。这种分类器就叫做<strong>硬投票分类器</strong></p>
<blockquote>
<p>每一个分类器都在同一个数据集上训练，导致其很可能会发生这样的错误。他们可能会犯同一种错误，所以也会有很多票投给了错误类别导致集成的准确率下降。</p>
</blockquote>
<p>所以应该用完全不同的算法得到多样的分类器，使它们会做出不同种类的错误，提高集成的正确率。</p>
<p>如果所有的分类器都能够预测类别的概率（有一个<code>predict_proba()</code>方法），可以让 sklearn 以最高的类概率来预测这个类，平均在所有的分类器上。这种方式叫做<strong>软投票</strong>。其表现比硬投票更好，因为给予高自信的投票更大的权重。</p>
<h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p><strong>有放回采样被称为装袋（<em>Bagging</em>，是 <em>bootstrap aggregating</em> 的缩写）。无放回采样称为粘贴（<em>pasting</em>）</strong></p>
<p>Bagging 和 Pasting 都允许在多个分类器间对训练集进行多次采样，但只有 Bagging可以通过使用不同的训练算法去得到一些不同的分类器。 Pasting是对每一个分类器都使用相同的训练算法，但是在不同的训练集上去训练。</p>
<blockquote>
<p>当所有的分类器被训练后，集成可以通过对所有分类器结果的简单聚合来对新的实例进行预测。<strong>聚合函数</strong>通常对分类是<em>统计模式</em>（例如硬投票分类器）或者对回归是平均。</p>
<p>每一个单独的分类器在如果在原始训练集上都是高偏差，但是聚合降低了偏差和方差。通常情况下，集成的结果是有一个相似的偏差，但是对比与在原始训练集上的单一分类器来讲有更小的方差。</p>
</blockquote>
<p>如果基分类器可以预测类别概率（例如它拥有<code>predict_proba()</code>方法），那么<code>BaggingClassifier</code>会自动的运行软投票。</p>
<h4 id="Out-of-bag"><a href="#Out-of-bag" class="headerlink" title="Out-of-bag"></a>Out-of-bag</h4><p>对于 Bagging 来说，一些实例可能被一些分类器重复采样，但其他的有可能不会被采样。<code>BaggingClassifier</code>默认是有放回的采样<code>m</code>个实例 （<code>bootstrap=True</code>）。意味着平均下来只有63%的训练实例被每个分类器采样，剩下的37%个没有被采样的训练实例就叫做 <em>Out-of-Bag</em> 实例。注意对于每一个的分类器它们的 37% 不是相同的。</p>
<p>可以拿出每一个分类器的 oob 来评估集成本身。</p>
<h4 id="随机贴片与随机子空间"><a href="#随机贴片与随机子空间" class="headerlink" title="随机贴片与随机子空间"></a>随机贴片与随机子空间</h4><p><code>BaggingClassifier</code>也支持采样特征。</p>
<p>对训练实例和特征的采样被叫做随机贴片。保留了所有的训练实例（例如<code>bootstrap=False</code>和<code>max_samples=1.0</code>），但是对特征采样（<code>bootstrap_features=True</code>并且/或者<code>max_features</code>小于 1.0）叫做随机子空间。</p>
<p><strong>采样特征导致更多的预测多样性，用高偏差换低方差。</strong></p>
<h3 id="随机森林（一种bagging模型）"><a href="#随机森林（一种bagging模型）" class="headerlink" title="随机森林（一种bagging模型）"></a>随机森林（一种bagging模型）</h3><p>随机森林算法在树生长时引入了额外的随机；与在节点分裂时需要找到最好分裂特征相反（详见第六章），它<strong>在一个随机的特征集中找最好的特征，</strong>导致了树的差异性，<strong>再一次用高偏差换低方差</strong>，总的来说是一个更好的模型。</p>
<h4 id="极端随机树"><a href="#极端随机树" class="headerlink" title="极端随机树"></a>极端随机树</h4><p>在随机森林上生长树时，在每个结点分裂时只考虑随机特征集上的特征。可以通过对特征使用随机阈值使树更加随机。<strong>这种极端随机的树被称为Extremely Randomized Trees简称为 <em>Extra-Tree</em>。</strong></p>
<blockquote>
<p>因为在每个节点上找到每个特征的最佳阈值是生长树最耗时的任务之一，所以 <em>Extra-Tree</em> 比规则的随机森林训练更快。</p>
</blockquote>
<h4 id="特征重要度"><a href="#特征重要度" class="headerlink" title="特征重要度"></a>特征重要度</h4><p><strong>重要的特征会出现在单一决策树更靠近根部的位置，而不重要的特征会经常出现在靠近叶子的位置。因此可以通过计算一个特征在森林的全部树中出现的平均深度来预测特征的重要性。</strong></p>
<p>随机森林可以非常方便快速得了解哪些特征实际上是重要的，特别是你需要进行特征选择的时候。</p>
<h3 id="提升"><a href="#提升" class="headerlink" title="提升"></a>提升</h3><p>提升（Boosting，最初称为<em>假设增强</em>）指的是可以将几个弱学习者组合成强学习者的集成方法。对于大多数的提升方法的思想就是按顺序去训练分类器，每一个都要尝试修正前面的分类。现如今已经有很多的提升方法了，但最著名的就是 <em>Adaboost</em>（适应性提升，是 <em>Adaptive Boosting</em> 的简称） 和 <em>Gradient Boosting</em>（梯度提升）。</p>
<h4 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h4><p>使一个新的分类器去修正之前分类结果的方法就是<strong>对之前分类结果错误的训练实例多加关注</strong>。这导致新的预测因子越来越多地聚焦于这种情况。这是 <em>Adaboost</em> 使用的技术。</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_7/7-7.png" alt="图7-7"></p>
<blockquote>
<p>序列学习技术的一个重要的<strong>缺点</strong>是：它不能被并行化（只能按步骤），因为每个分类器只能在之前的分类器已经被训练和评价后再进行训练。</p>
</blockquote>
<p><strong>Adaboost的算法</strong></p>
<p>每一个实例的权重<code>wi</code>初始都被设为<code>1/m</code></p>
<p>第一个分类器被训练后，在训练集上算出权重误差率<code>r1</code>：</p>
<p><strong>第j个分类器的权重误差率：</strong></p>
<script type="math/tex; mode=display">
r_j\ = \ \frac{\sum_{i=1,\hat y_j^{(i)}\neq y^{(i)}}^m \omega^{(i)}}{\sum _{i=1}^m \omega^{(i)}}</script><p>$\hat y_j^i$ 是第j个分类器对于第i个实例的预测。</p>
<p><strong>分类器的权重</strong></p>
<p>其中<code>η</code>是超参数学习率（默认为 1）。分类器准确率越高，它的权重就越高。</p>
<script type="math/tex; mode=display">
\alpha_j = \eta \log \frac{1-r_j}{r_j}</script><p><strong>权重更新规则</strong></p>
<p>对于<code>i=1, 2, ..., m</code></p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_7/E7-3.png" alt="公式7-3"></p>
<p>随后所有实例的权重都被归一化。</p>
<p>为了进行预测，Adaboost 通过分类器权重 $α_{j}$简单的计算了所有的分类器和权重。预测类别会是权重投票中主要的类别。</p>
<p><strong>Adaboost 分类器</strong></p>
<script type="math/tex; mode=display">
\hat y(x)\ = \ {argmax}_k \ \ \sum^N_{j=1,\hat y_j(x)=k} \alpha_j</script><p>其中<code>N</code>是分类器的数量。</p>
<blockquote>
<p>sklearn 通常使用 Adaboost 的多分类版本 <em>SAMME</em>（<em>分段加建模使用多类指数损失函数</em>）。如果只有两类别，那么 <em>SAMME</em> 是与 Adaboost 相同的。如果分类器可以预测类别概率（有<code>predict_proba()</code>），如果 sklearn 可以使用 <em>SAMME</em> 叫做<code>SAMME.R</code>的变量（R 代表“REAL”），这种依赖于类别概率的通常比依赖于分类器的更好。</p>
</blockquote>
<p>如果 Adaboost 集成<strong>过拟合</strong>，可以尝试<strong>减少基分类器的数量</strong>或者<strong>对基分类器使用更强的正则化</strong>。</p>
<h4 id="梯度提升"><a href="#梯度提升" class="headerlink" title="梯度提升"></a>梯度提升</h4><p>梯度提升也是通过向集成中逐步增加分类器运行的，每一个分类器都修正之前的分类结果。然而，并不像 Adaboost 那样每一次迭代都更改实例的权重，这个方法<strong>使用新的分类器去拟合前面分类器预测的<em>残差</em> </strong>。</p>
<p><strong>梯度提升回归树</strong>（GBRT，<em>Gradient Tree Boosting</em> 或者 <em>Gradient Boosted Regression Trees</em>）</p>
<p><strong>梯度提升决策树</strong> (GBDT， <em>Gradient Boosted Decision Trees</em>)是梯度提升(GB)算法限定基学习器是<strong>回归决策树时的模型，尤其是CART回归树</strong>。</p>
<p>sklean 中的<code>GradientBoostingRegressor</code>可以用来训练 GBRT 集成，也有超参数去控制集成训练，例如基分类器的数量（<code>n_estimators</code>）。超参数<code>learning_rate</code> 确立了每个树的贡献。</p>
<blockquote>
<p>如果把<code>learning_rate</code>设置为一个很小的树，例如 0.1，在集成中就需要更多的树去拟合训练集，但预测通常会更好。这个正则化技术叫做 <strong><em>shrinkage</em></strong>。</p>
</blockquote>
<h5 id="Early-stop-早停"><a href="#Early-stop-早停" class="headerlink" title="Early stop(早停)"></a>Early stop(早停)</h5><p><code>GradientBoostingRegressor</code>有方法<code>staged_predict()</code>：它在训练的每个阶段返回一个迭代器。</p>
<p>与先在一大堆树中训练，然后再回头去找最优数目相反。可以通过设置<code>warm_start=True</code>来实现 早停，这使得当<code>fit()</code>方法被调用时 sklearn 保留现有树，并允许增量训练。</p>
<p><code>GradientBoostingRegressor</code>也支持指定用于训练每棵树的训练实例比例的超参数<code>subsample</code>。例如如果<code>subsample=0.25</code>，那么每个树都会在 25% 随机选择的训练实例上训练。这也是个高偏差换低方差的作用。同样也加速了训练。这个技术叫做<strong><em>随机梯度提升</em></strong>。</p>
<h4 id="极端梯度提升（XGBoost）"><a href="#极端梯度提升（XGBoost）" class="headerlink" title="极端梯度提升（XGBoost）"></a>极端梯度提升（XGBoost）</h4><ol>
<li>提升树只使用了一阶泰勒展开，而XGBoost使用了二阶泰勒展开。</li>
<li>xgboost在<strong>代价函数里加入了正则项，用于控制模型的复杂度</strong>。</li>
<li><strong>列抽样（column subsampling）</strong>。xgboost借鉴了随机森林的做法，支持列抽样（即每次的输入特征不是全部特征），不仅能降低过拟合，还能减少计算</li>
<li><strong>并行化处理</strong>：在训练之前，预先对每个特征内部进行了排序找出候选切割点，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。</li>
</ol>
<p>xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。例如，xgboost支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）</p>
<p><a href="https://zhuanlan.zhihu.com/p/57814935" target="_blank" rel="noopener">参考</a></p>
<h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><p><strong><em>Stacking</em>（<em>stacked generalization</em> 的缩写），这个算法不使用琐碎的函数（如硬投票）来聚合集合中所有分类器的预测，我直接训练一个模型来执行这个聚合。</strong></p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_7/7-12.png" alt="图7-12"></p>
<p>为了训练这个 <em>blender</em> ，一个通用的方法是<strong>采用保持集</strong>。</p>
<ol>
<li>首先，训练集被分为两个子集，第一个子集被用作训练第一层。</li>
<li>第一层的分类器被用来预测第二个子集（保持集）。</li>
<li>使用这些预测结果作为输入特征来创建一个新的训练集，保持目标数值不变，随后 <em>blender</em> 在这个新的训练集上训练。</li>
</ol>
<p>sklearn 并不直接支持 stacking ，但可以使用开源项目<a href="https://github.com/Menelau/DESlib" target="_blank" rel="noopener">DESlib</a>（基于sklearn）</p>
<h3 id="练习-1"><a href="#练习-1" class="headerlink" title="练习"></a>练习</h3><ol>
<li><p>如果你在相同训练集上训练 5 个不同的模型，它们都有 95% 的准确率，那么你是否可以通过组合这个模型来得到更好的结果？如果可以那怎么做呢？如果不可以请给出理由。</p>
<blockquote>
<p>可以通过集成学习将5个模型集成在一起</p>
<p>通过投票的方式预测</p>
</blockquote>
</li>
<li><p>软投票和硬投票分类器之间有什么区别？</p>
<blockquote>
<p>硬投票分类器计算集合中每个分类器的投票，并选择得到最多选票的类。</p>
<p>软投票分类器计算每个类的平均估计类概率，并选择概率最高的类，也就是给置信度高的类更高的权重，但是前提是可以得到每个类的概率。</p>
</blockquote>
</li>
<li><p>是否有可能通过分配多个服务器来加速 bagging 集成系统的训练？pasting 集成，boosting 集成，随机森林，或 stacking 集成怎么样？</p>
<blockquote>
<p>bagging，pasting和stacking都可以并行，随机森林算bagging</p>
<p>boosting是序列学习技术，不能并行。</p>
</blockquote>
</li>
<li><p>out-of-bag 评价的好处是什么？</p>
<blockquote>
<p>包外集成中的每个预测器都使用它没有经过训练的实例进行评估，不需要额外的验证集。</p>
</blockquote>
</li>
<li><p>是什么使 Extra-Tree 比规则随机森林更随机呢？这个额外的随机有什么帮助呢？那这个 Extra-Tree 比规则随机森林谁更快呢？</p>
<blockquote>
<p>Extra-Tree在随机森林上生长树时，在每个结点分裂时只考虑随机特征集上的特征，额不是在每个节点上找到每个特征的最佳阈值，所以训练时 <em>Extra-Tree</em> 比规则的随机森林更快。</p>
</blockquote>
</li>
<li><p>如果你的 Adaboost 模型欠拟合，那么你需要怎么调整超参数？</p>
<blockquote>
<p>如果 Adaboost 集成<strong>过拟合</strong>，可以尝试<strong>减少基分类器的数量</strong>或者<strong>对基分类器使用更强的正则化</strong>。</p>
</blockquote>
</li>
<li><p>如果你的梯度提升过拟合，那么你应该调高还是调低学习率呢？</p>
<blockquote>
<p>降低学习率，或使用早停的方法来寻找正确数量的基学习器。</p>
</blockquote>
</li>
<li><p>导入 MNIST 数据（第三章中介绍），把它切分进一个训练集，一个验证集，和一个测试集（例如 40000 个实例进行训练，10000 个进行验证，10000 个进行测试）。然后训练多个分类器，例如一个随机森林分类器，一个 Extra-Tree 分类器和一个 SVM。接下来，尝试将它们组合成集成，使用软或硬投票分类器来胜过验证集上的所有集合。一旦找到了，就在测试集上实验。与单个分类器相比，它的性能有多好？</p>
</li>
<li><p>从练习 8 中运行个体分类器来对验证集进行预测，并创建一个新的训练集并生成预测：每个训练实例是一个向量，包含来自所有分类器的图像的预测集，目标是图像类别。祝贺你，你刚刚训练了一个 <em>blender</em> ，和分类器一起组成了一个叠加组合！现在让我们来评估测试集上的集合。对于测试集中的每个图像，用所有分类器进行预测，然后将预测馈送到 <em>blender</em> 以获得集合的预测。它与你早期训练过的投票分类器相比如何？</p>
</li>
</ol>
<hr>
<h2 id="第08章-降维"><a href="#第08章-降维" class="headerlink" title="第08章 降维"></a>第08章 降维</h2><blockquote>
<p>警告：降维会丢失一些信息，因此即使这种方法可以加快训练的速度，同时也会让系统表现的稍微差一点。降维会让工作流水线更复杂因而更难维护。所有应该先尝试使用原始的数据来训练，如果训练速度太慢的话再考虑使用降维。在某些情况下，降低训练集数据的维度可能会筛选掉一些噪音和不必要的细节，这可能会让结果比降维之前更好（这种情况通常不会发生；它只会加快训练的速度）。</p>
</blockquote>
<p>降维除了可以加快训练速度外，在数据可视化方面（或者 DataViz）也十分有用。降低特征维度到 2（或者 3）维从而可以在图中画出一个高维度的训练集，让我们可以通过视觉直观的发现一些非常重要的信息，比如聚类。</p>
<p><strong>两种主要的降维方法：投影（projection）和流形学习（Manifold Learning），三种流行的降维技术：主成分分析（PCA），核主成分分析（Kernel PCA）和局部线性嵌入（LLE）。</strong></p>
<h3 id="维数灾难（curse-of-dimentionality）"><a href="#维数灾难（curse-of-dimentionality）" class="headerlink" title="维数灾难（curse of dimentionality）"></a>维数灾难（curse of dimentionality）</h3><p>一个 1,0000 维的单位超正方体中，随机选的点离所有边界大于 0.001（靠近中间位置）的概率（$1-0.998^{10000}$），近似于1，在高维超正方体中，大多数点都分布在边界处。</p>
<p>在一个 1,000,000 维超立方体中随机抽取两点的平均距离，大概为 408.25（大致 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-50eeccb6ef846e2d0af5daef5cab1fa0.gif" target="_blank" rel="noopener"><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-50eeccb6ef846e2d0af5daef5cab1fa0.gif" alt="\sqrt{1,000,000/6}"></a>）</p>
<p>表明高维数据集有很大风险分布的非常稀疏：大多数训练实例可能彼此远离。也意味着一个新实例可能远离任何训练实例，使得预测的可靠性远低于较低维度数据的预测，因为它们将基于更大的推测（extrapolations）。简而言之，训练集的维度越高，过拟合的风险就越大。</p>
<p>理论上来说，维数爆炸的一个解决方案是增加训练集的大小从而达到拥有足够密度的训练集。</p>
<h3 id="降维的主要方法"><a href="#降维的主要方法" class="headerlink" title="降维的主要方法"></a>降维的主要方法</h3><h4 id="投影（Projection）"><a href="#投影（Projection）" class="headerlink" title="投影（Projection）"></a>投影（Projection）</h4><p>大多数现实生活的问题中，训练实例并不是在所有维度上均匀分布的，许多特征几乎是常数，而其他特征则高度相关（如前面讨论的 MNIST）。<strong>所有训练实例实际上位于（或接近）高维空间的低维子空间内。</strong></p>
<p>投影并不总是降维的最佳方法。在很多情况下，子空间可能会扭曲和转动。</p>
<h4 id="流形学习（Manifold-Learning）"><a href="#流形学习（Manifold-Learning）" class="headerlink" title="流形学习（Manifold Learning）"></a>流形学习（Manifold Learning）</h4><p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_8/8-4.jpeg" alt="img"></p>
<p>瑞士卷一个是二维流形的例子。简而言之，二维流形是一种二维形状，它可以在更高维空间中弯曲或扭曲。一个<code>d</code>维流形是类似于<code>d</code>维超平面的<code>n</code>维空间（其中<code>d &lt; n</code>）的一部分。瑞士卷有些像 2D 平面，但是它实际上是在第三维中卷曲。</p>
<p><strong>通过对训练实例所在的流形进行建模从而达到降维目的，叫做流形学习。</strong>它依赖于流形猜想（manifold assumption），也被称为流形假设（manifold hypothesis），认为大多数现实世界的高维数据集大都靠近一个更低维的流形。</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_8/8-6.jpeg" alt="img"></p>
<p>如果在训练模型之前降低训练集的维数，那训练速度肯定会加快，但并不总是会得出更好的训练效果；这一切都取决于数据集。</p>
<h3 id="降维技术"><a href="#降维技术" class="headerlink" title="降维技术"></a>降维技术</h3><h4 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h4><p>主成分分析（Principal Component Analysis）是目前为止最流行的降维算法。首先它找到接近数据集分布的超平面，然后将所有的数据都投影到这个超平面上。</p>
<h5 id="保留最大方差"><a href="#保留最大方差" class="headerlink" title="保留最大方差"></a>保留最大方差</h5><p>选择保持最大方差的轴将原始数据集投影到该轴上的均方距离最小，损失更少的信息。</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_8/8-7.jpeg" alt="img"></p>
<h5 id="主成分（Principle-Componets）"><a href="#主成分（Principle-Componets）" class="headerlink" title="主成分（Principle Componets）"></a>主成分（Principle Componets）</h5><p> 定义第<code>i</code>个轴的单位矢量被称为第<code>i</code>个主成分（PC）。</p>
<p><strong>奇异值分解（SVD）</strong>的标准矩阵分解：将训练集矩阵<code>X</code>分解为三个矩阵<code>U·Σ·V^T</code>的点积，其中<code>V^T</code>包含我们想要的所有主成分，</p>
<blockquote>
<p>PCA 假定数据集以原点为中心。Scikit-Learn 的<code>PCA</code>类负责数据集中心化处理。但是，如果自己实现 PCA，或者使用其他库，首先要先对数据做中心化处理。</p>
</blockquote>
<h5 id="投影到d维空间"><a href="#投影到d维空间" class="headerlink" title="投影到d维空间"></a>投影到<code>d</code>维空间</h5><script type="math/tex; mode=display">
X_{d-proj} = X\cdot W_d</script><p>$W_d$定义为包含前<code>d</code>个主成分的矩阵（即由<code>V^T</code>的前<code>d</code>列组成的矩阵）</p>
<h5 id="方差解释率（Explained-Variance-Ratio）"><a href="#方差解释率（Explained-Variance-Ratio）" class="headerlink" title="方差解释率（Explained Variance Ratio）"></a>方差解释率（Explained Variance Ratio）</h5><p>每个主成分的方差解释率，可通过<code>explained_variance_ratio_</code>变量获得。它表示位于每个主成分轴上的数据集方差的比例。</p>
<h5 id="选择正确的维度"><a href="#选择正确的维度" class="headerlink" title="选择正确的维度"></a>选择正确的维度</h5><p>倾向于选择加起来到方差解释率能够达到足够占比（例如 95%）的维度的数量，而不是任意选择要降低到的维度数量。</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_8/8-8.jpeg" alt="img"></p>
<h5 id="PCA压缩"><a href="#PCA压缩" class="headerlink" title="PCA压缩"></a>PCA压缩</h5><p>原始数据和重构数据之间的均方距离（压缩然后解压缩）被称为重构误差（reconstruction error）。</p>
<p>PCA逆变换</p>
<script type="math/tex; mode=display">
X_{recovered} = X_{d-proj}\cdot W_d^T</script><h5 id="增量-PCA（Incremental-PCA）"><a href="#增量-PCA（Incremental-PCA）" class="headerlink" title="增量 PCA（Incremental PCA）"></a>增量 PCA（Incremental PCA）</h5><p>有些算法如SVD需要在内存中处理整个训练集。可以将训练集分批，一次只对一个批量使用 IPCA 算法。这对大型训练集非常有用，并且可以在线应用 PCA（即在新实例到达时即时运行）。</p>
<h5 id="随机-PCA（Randomized-PCA）"><a href="#随机-PCA（Randomized-PCA）" class="headerlink" title="随机 PCA（Randomized PCA）"></a>随机 PCA（Randomized PCA）</h5><p>这是一种随机算法，可以快速找到前d个主成分的近似值。它的计算复杂度是<code>O(m × d^2) + O(d^3)</code>，而不是<code>O(m × n^2) + O(n^3)</code>，所以当<code>d</code>远小于<code>n</code>时，它比之前的算法快得多。</p>
<h4 id="核-PCA（Kernel-PCA）"><a href="#核-PCA（Kernel-PCA）" class="headerlink" title="核 PCA（Kernel PCA）"></a>核 PCA（Kernel PCA）</h4><p>核技巧，一种将实例隐式映射到非常高维空间（称为特征空间）的数学技术，让支持向量机可以应用于非线性分类和回归。高维特征空间中的线性决策边界对应于原始空间中的复杂非线性决策边界。</p>
<p>同样的技巧可以应用于 PCA，从而可以执行复杂的非线性投影来降低维度。这就是所谓的核 PCA（kPCA）。它通常能够很好地保留投影后的簇，有时甚至可以展开分布近似于扭曲流形的数据集。</p>
<p><strong>如何选择一种核并调整参数</strong></p>
<blockquote>
<ol>
<li>网格搜索</li>
<li>非监督学习</li>
</ol>
</blockquote>
<h4 id="局部线性嵌入（LLE）"><a href="#局部线性嵌入（LLE）" class="headerlink" title="局部线性嵌入（LLE）"></a>局部线性嵌入（LLE）</h4><p>局部线性嵌入（Locally Linear Embedding）是一种非线性降维（NLDR）方法。这是一种流形学习技术。</p>
<p>LLE 首先测量每个训练实例与其最近邻（c.n.）之间的线性关系，然后寻找能最好地保留这些局部关系的训练集的低维表示。</p>
<blockquote>
<p>擅长展开没有太多噪音的扭曲的流形。</p>
</blockquote>
<h5 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h5><p>对于每个训练实例 $x^{(i)}$，该算法识别其最近的<code>k</code>个邻居，然后尝试将 $x^{(i)}$重构为这些邻居的线性函数。找到权重 $w_{i,j}$从而使  $x^{(i)}$和 $\sum_{j=1}^{m}w_{i,j} x^{(j)}$之间的平方距离尽可能的小，假设如果 $x^{(j)}$不是 $x^{(i)}$的<code>k</code>个最近邻时 $w_{i,j}=0$。</p>
<p>第一步：对局部关系进行线性建模 </p>
<p>$W=argmin_w\ \sum_{i=1}^m | x^{(i)}-\sum_{j=1}^m w_{i,j}x^{(j)}| ^2 \\ subject \ to \ \left \{  \begin{array}{cc} w_{i,j}=0&amp;\ \ if \ x^{j} \  is \ not \ one \ of \ the \ k \ c.n. \ of \ x^{i} \\ \sum_{j=1}^m w_{i,j}=1&amp; for\ i=1,2, \cdots ,m\end{array}\right.$</p>
<p>第二个约束简单地对每个训练实例 $x^{(i)}$的权重进行归一化。</p>
<p>权重矩阵 $\widehat{W}$包含权重 $\hat{w_{i,j}}$]对训练实例的线形关系进行编码。第二步是将训练实例投影到一个<code>d</code>维空间（<code>d &lt; n</code>）中去，同时尽可能的保留这些局部关系。如果 $z^{(i)}$是 $x^{(i)}$]在这个<code>d</code>维空间的图像，那么需要 $z^{(i)}$和 $\sum_{j=1}^{m}\hat{w_{i,j}}\ z^{(j)}$之间的平方距离尽可能的小。看起来与第一步非常相似，但不是保持实例固定并找到最佳权重，而是<strong>保持权重不变，并在低维空间中找到实例图像的最佳位置</strong>。<code>Z</code>是包含所有 $z^{(i)}$]的矩阵。</p>
<p>第二步：保持关系的同时进行降维</p>
<p>$W=argmin_z\ \sum_{i=1}^m | z^{(i)}-\sum_{j=1}^m w_{i,j}z^{(j)}| ^2 $</p>
<p>Scikit-Learn 的 LLE 实现具有如下的计算复杂度：查找<code>k</code>个最近邻为<code>O(m log(m) n log(k))</code>，优化权重为<code>O(m n k^3)</code>，建立低维表示为<code>O(d m^2)</code>。</p>
<h4 id="其他降维方法"><a href="#其他降维方法" class="headerlink" title="其他降维方法"></a>其他降维方法</h4><ul>
<li>多维缩放（MDS）在尝试保持实例之间距离的同时降低了维度</li>
<li>Isomap 通过将每个实例连接到最近的邻居来创建图形，然后在尝试保持实例之间的测地距离时降低维度。</li>
<li>t-分布随机邻域嵌入（t-Distributed Stochastic Neighbor Embedding，<strong>t-SNE</strong>）可以用于降低维度，同时试图保持相似的实例临近并将不相似的实例分开。它主要用于可视化，尤其是用于可视化高维空间中的实例（例如，可以将MNIST图像降维到 2D 可视化）。</li>
<li>线性判别分析（Linear Discriminant Analysis，LDA）实际上是一种分类算法，但在训练过程中，它会学习类之间最有区别的轴，然后使用这些轴来定义用于投影数据的超平面。LDA 的好处是投影会尽可能地保持各个类之间距离，所以在运行另一种分类算法（如 SVM 分类器）之前，LDA 是很好的降维技术。</li>
</ul>
<h3 id="练习-2"><a href="#练习-2" class="headerlink" title="练习"></a>练习</h3><ol>
<li><p>减少数据集维度的主要动机是什么？主要缺点是什么？</p>
<blockquote>
<p>动机：</p>
<ol>
<li>加快训练速度(筛选掉一些噪音和不必要的细节，可能会让结果比降维之前更好)</li>
<li>数据可视化方面：降低特征维度到 2（或者 3）维从而可以在图中画出一个高维度的训练集，可以通过视觉直观的发现一些非常重要的信息</li>
<li>节省空间</li>
</ol>
<p>缺点：</p>
<ol>
<li>丢失一些信息，降低算法后续性能</li>
<li>使工作流水线更复杂因而更难维护</li>
<li>降低可解释性</li>
</ol>
</blockquote>
</li>
<li><p>什么是维度爆炸？</p>
<blockquote>
<p>维度爆炸是指高维空间中出现了许多低维空间不存在的问题，在机器学习中，一个常见的表现是，随机采样的高维向量通常非常稀疏，增加了过度拟合的风险，使得在没有大量训练数据的情况下很难识别数据中的模式。</p>
</blockquote>
</li>
<li><p>一旦对某数据集降维，我们可能恢复它吗？如果可以，怎样做才能恢复？如果不可以，为什么？</p>
<blockquote>
<p>一旦数据集通过降维降到了一个较小的维度，就几乎不会完全复现，因为在维度减小的过程中，损失了一部分信息，PCA有逆过程，可以重构一个和原始数据集比较相似的数据集，但是t-SNE没有。</p>
</blockquote>
</li>
<li><p>PCA 可以用于降低一个高度非线性对数据集吗？</p>
<blockquote>
<p>PCA可以显著降低大多数数据集的维数，即使他们是高度非线性的，因为它至少可以消除无用的维度，然而如果没有无用的维度，例如”瑞士卷”，使用PCA降维将会损失许多信息。</p>
</blockquote>
</li>
<li><p>假设你对一个 1000 维的数据集应用 PCA，同时设置方差解释率为 95%，你的最终数据集将会有多少维？</p>
<blockquote>
<p>取决于数据集</p>
<p>如果数据集由几乎完全对齐的点组成，该情况下，PCA可以把数据降到一维，并保持95%的方差结实率</p>
<p>如果数据集中的点完全随机，则该情况下，需要降到所有维度，并且每一维度都有95%的方差。</p>
<p><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_8/8-8.jpeg" alt="img"></p>
</blockquote>
</li>
<li><p>在什么情况下你会使用普通的 PCA，增量 PCA，随机 PCA 和核 PCA？</p>
<blockquote>
<p>一般默认使用普通的PCA，但它只在数据集适合内存的情况下工作。</p>
<p>增量PCA对于不适合内存的大型数据集有用，可以分批训练，一次只对一个批量使用 IPCA 算法。但它比常规PCA要慢，所以如果数据集适合内存应该选择常规的PCA。增量PCA也适用于在线任务，当不断有新实例到达，需要动态应用PCA时。</p>
<p>随机PCA：当想要大大减少维度并且数据集适合时，它比普通PCA快得多。</p>
<p>数据呈现复杂的高维非线性时，使用核PCA</p>
</blockquote>
</li>
<li><p>你该如何评价你的降维算法在你数据集上的表现？</p>
<blockquote>
<p>一个降维算法在不丢失太多信息的情况下从数据集中消除了大量的维数，那么它的性能就会很好。</p>
<p>一种测量方法是应用反向变换并测量重构误差。然而，并不是所有的降维算法都提供了反向转换。</p>
<p>如果将降维作为另一种机器学习算法前的预处理步骤，那么可以简单地测量第二种算法的性能;如果降维不会丢失太多信息，那么算法的性能应该与使用原始数据集时一样好。</p>
</blockquote>
</li>
<li><p>将两个不同的降维算法串联使用有意义吗？</p>
<blockquote>
<p>有</p>
<p>一个常见的例子是使用PCA快速摆脱大量无用的维数，然后应用另一种慢得多的降维算法，如LLE。这种分两步的方法可能会产生与仅使用LLE相同的性能，但大大缩短了时间。</p>
</blockquote>
</li>
<li><p>加载 MNIST 数据集（在第 3 章中介绍），并将其分成一个训练集和一个测试集（将前 60,000 个实例用于训练，其余 10,000 个用于测试）。在数据集上训练一个随机森林分类器，并记录了花费多长时间，然后在测试集上评估模型。接下来，使用 PCA 降低数据集的维度，设置方差解释率为 95%。在降维后的数据集上训练一个新的随机森林分类器，并查看需要多长时间。训练速度更快？接下来评估测试集上的分类器：它与以前的分类器比较起来如何？</p>
</li>
<li><p>使用 t-SNE 将 MNIST 数据集缩减到二维，并使用 Matplotlib 绘制结果图。您可以使用 10 种不同颜色的散点图来表示每个图像的目标类别。或者，您可以在每个实例的位置写入彩色数字，甚至可以绘制数字图像本身的降维版本（如果绘制所有数字，则可视化可能会过于混乱，因此您应该绘制随机样本或只在周围没有其他实例被绘制的情况下绘制）。你将会得到一个分隔良好的的可视化数字集群。尝试使用其他降维算法，如 PCA，LLE 或 MDS，并比较可视化结果。</p>
</li>
</ol>
<hr>
<h2 id="第09章-非监督学习"><a href="#第09章-非监督学习" class="headerlink" title="第09章 非监督学习"></a>第09章 非监督学习</h2><p>降维就是最常用的非监督机器学习方法之一。</p>
<h3 id="聚类（clustering）"><a href="#聚类（clustering）" class="headerlink" title="聚类（clustering）"></a>聚类（clustering）</h3><p>将相似的实例聚集在一起</p>
<p><strong>和分类的区别：分类问题有标签，而聚类问题没有标签。</strong></p>
<p><strong>应用场景：</strong></p>
<p>降维，半监督学习，图像分类，推荐系统，搜索引擎，客户细分等</p>
<p>也可以用于异常检测：数据聚类后，和所有cluster相聚很远的数据点可能是有问题的。</p>
<p>半监督学习：只有若干数据有标签，可以聚类后将标签传递给同一cluster</p>
<p>搜索引擎：例如搜索图片，将所有图片聚类，然后根据用户上传的图片，判断在哪一个cluster。</p>
<p>image segmentation：数据监测，追踪，监测物体的边界</p>
<blockquote>
<p> 参考：<a href="https://www.cnblogs.com/LittleHann/p/6595148.html" target="_blank" rel="noopener">https://www.cnblogs.com/LittleHann/p/6595148.html</a></p>
<p>​            <a href="https://zhuanlan.zhihu.com/p/150333968" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/150333968</a></p>
</blockquote>
<p>一些算法有中心：centroid （K-Means）</p>
<p>一些算法分层：hierarchical</p>
<p>一些算法基于密度：DBSCAN</p>
<p>一些算法基于概率：GMM（高斯混合模型）</p>
<h4 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h4><p>1957年，Stuart Lloyd在Bell Labs提出。该算法一般是最快的聚类方法，且一定会收敛，不会永远震荡。</p>
<p><strong>计算复杂度</strong>：一般情况下（数据具有clustering structure）和实例的个数m以及类的个数k呈线性关系。否则，最坏的情况，指数增长（一般很少发生）。</p>
<p><strong>算法：</strong></p>
<ol>
<li>初始化中心，然后将数据分到最近邻的类中。</li>
<li>把所有实例分到相应的类中，计算数据中心</li>
<li>按照新的中心，重新将实例分到相应的类中，计算数据中心</li>
<li>重复2,3，知道中心不发生变化</li>
</ol>
<p><strong>hard-clustering和soft-clustering</strong></p>
<p>hard-clustering将实例分到一个cluster中</p>
<p>soft-clustering给出每个实例在每一个cluster中的评分，可以是距离，或者RBF等等</p>
<p><strong>缺点：</strong></p>
<p>虽然K-Means一定会收敛，但是可能会收敛到局部最优解，和初始中心的设置有很大的关系。</p>
<p><strong>解决办法：</strong></p>
<p>初始化聚类中心的方法：</p>
<ol>
<li><p>提前知道大概的中心位置，可以用<code>init</code>参数传递给KMeans</p>
</li>
<li><p>多次运行kmeans，保留最好的结果。<code>n_init，</code>sklearn中默认为10，用intertia（所有实例距离其最近邻聚类中心的距离的平方和）判定好坏（越小越好）</p>
<blockquote>
<p>sklearn中的score遵循“greater is better”的准则，所以score是intertia的负数。</p>
</blockquote>
</li>
<li><p>改进算法：<strong>K-Means++</strong>（2006年，由David Arthur and Sergei Vassilvitskii提出）</p>
<blockquote>
<p>初始化聚类中心尽可能彼此远离，可以有效避免局部最优解。</p>
<ol>
<li>随机选取第一个中心$c^{(1)}$</li>
<li>以概率$D(x^{(i)})^2/\sum_{j=1}^m D(x^{(j)})^2$ 选取实例作为新的中心$c^{(i)}$，$D(x^{(i)})$表示实例距离已经选定的中心的距离。</li>
<li>重复2直到确定所有中心</li>
</ol>
</blockquote>
<p>sklearn中默认使用这种算法，可以通过设置<code>init=random</code>使用原生K-Means</p>
</li>
</ol>
<h4 id="加速K-Means算法"><a href="#加速K-Means算法" class="headerlink" title="加速K-Means算法"></a>加速K-Means算法</h4><blockquote>
<p>由Charles Elkan于2003年提出。利用<strong>三角不等式的性质（triangle inequality），记录实例和聚类中心距离的上限和下限（lower and upper bounds）加速K-Means，</strong>避免许多无用的距离计算.</p>
<p>sklearn中默认使用这种算法。</p>
</blockquote>
<h4 id="mini-batch-K-Means算法"><a href="#mini-batch-K-Means算法" class="headerlink" title="mini-batch K-Means算法"></a>mini-batch K-Means算法</h4><blockquote>
<p>2000年由David Sculley提出，可以处理内存处理不下的数据（也可以memmap），与regular K-Means相比，快3-4倍，但intertia较差。</p>
<p>sklearn中有MiniBatchKMeans类。</p>
</blockquote>
<h4 id="确定最佳类的个数"><a href="#确定最佳类的个数" class="headerlink" title="确定最佳类的个数"></a>确定最佳类的个数</h4><p>不能用intertia，因为类越多，intertia越小。选取intertia-k的肘部点。</p>
<p><img src="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0908.png" alt="mls2 0908"></p>
<h4 id="silhouette-score（平均轮廓系数）"><a href="#silhouette-score（平均轮廓系数）" class="headerlink" title="silhouette score（平均轮廓系数）"></a><em>silhouette score（平均轮廓系数）</em></h4><p>silhouette score：所有实例silhouette efficient（轮廓系数）的平均。</p>
<p>轮廓系数（-1~1）：(b – a) / max(a, b)，a是同类中其他实例距离的平均（the mean intra-cluster distance），b是到最近邻类中其他实例的平均距离（the mean nearest-cluster distance ），接近+1,意味着实例都处于当前类中，接近-1，意味着实例被分类到了错误的类，接近0，意味着数据点在边界。</p>
<p><img src="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0909.png" alt="mls2 0909"></p>
<p>还可以画出silhouette diagram：算出每个实例的轮廓系数，并将它们分到相应的类里：</p>
<p><img src="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0910.png" alt="mls2 0910"></p>
<p>红色虚线表示silhouette score，凡是处于虚线左侧的刀型图，表示该类的实例和其他类距离过近。</p>
<p>k=4和5中类的大小不一样，4中橙色的类明显更大，而5中类更均匀，所以即使4的silhouette score更大，看上去5的表现更好。</p>
<h4 id="K-Means的局限性"><a href="#K-Means的局限性" class="headerlink" title="K-Means的局限性"></a>K-Means的局限性</h4><ol>
<li><p>需要提前跑几次，防止陷入过拟合</p>
</li>
<li><p>需要自己确定类的个数</p>
</li>
<li><p>当类的大小不均一，或者密度不一，或者分布不是球形时，表现不好</p>
<blockquote>
<p>椭圆分布（elliptical clusters），Gaussian mixture model表现更好</p>
<p>运行K-Means之前，scale the feature</p>
</blockquote>
</li>
</ol>
<h4 id="K-Means的具体应用"><a href="#K-Means的具体应用" class="headerlink" title="K-Means的具体应用"></a>K-Means的具体应用</h4><p><strong>图像分割</strong> <em>Image segmentation</em> ：将图像分成若干部分。</p>
<blockquote>
<ol>
<li>语义分割（semantic segmentation）</li>
<li>实例分割（instance segmentation）</li>
</ol>
<p>一般基于CNN实现。</p>
</blockquote>
<p>颜色分割（color segmentation）：应用场景：从卫星图计算森林面积的大小。</p>
<p><strong>数据预处理</strong></p>
<p>监督学习前使用K-Means对数据进行聚类。降低了数据的维度，更重要的是聚类得到的特征更加线性可分。</p>
<p><strong>用于半监督学习</strong></p>
<p>当有很多没有标签的数据时，可以使用K-Means对有标签的数据进行聚类，将最接近中心的图片（或其他）（representative image）表示出来。将标签传递给类中其他数据，这个过程叫做label  propagation。</p>
<blockquote>
<p>注意：边界上的数据点可能被错误的分类了，所以标签传递最好传递给距离聚类中心最近的一部分数据（比如20%）</p>
</blockquote>
<h4 id="主动学习-active-learning"><a href="#主动学习-active-learning" class="headerlink" title="主动学习(active learning)"></a>主动学习(active learning)</h4><p>不确定度采样<code>Uncertainty Sampling</code></p>
<ol>
<li>用仅有的有标签数据训练模型，用该模型预测所有无标签数据</li>
<li>将预测结果中最不准确（概率最低）的实例拿出来给专家贴标签</li>
<li>重复上述过程</li>
</ol>
<p>更深入了解可以参考：<a href="https://blog.csdn.net/qq_39856931/article/details/106433187" target="_blank" rel="noopener">https://blog.csdn.net/qq_39856931/article/details/106433187</a></p>
<h4 id="KNN和K-Means的区别"><a href="#KNN和K-Means的区别" class="headerlink" title="KNN和K-Means的区别"></a>KNN和K-Means的区别</h4><div class="table-container">
<table>
<thead>
<tr>
<th><strong>KNN</strong></th>
<th><strong>K-Means</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1.KNN是分类算法  2.监督学习 3.喂给它的数据集是带label的数据，已经是完全正确的数据</td>
<td>1.K-Means是聚类算法  2.非监督学习 3.喂给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序</td>
</tr>
<tr>
<td>没有明显的前期训练过程，属于memory-based learning</td>
<td>有明显的前期训练过程</td>
</tr>
<tr>
<td>K的含义：来了一个样本x，要给它分类，即求出它的y，就从数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c</td>
<td>K的含义：K是人工固定好的数字，假设数据集合可以分为K个簇，由于是依靠人工定好，需要一点先验知识</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td>相似点：都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法，一般用KD树来实现NN。</td>
</tr>
</tbody>
</table>
</div>
<h4 id="DBSCAN（Density-Based-Spatial-Clustering-of-Applications-with-Noise）"><a href="#DBSCAN（Density-Based-Spatial-Clustering-of-Applications-with-Noise）" class="headerlink" title="DBSCAN（Density-Based Spatial Clustering of Applications with Noise）"></a>DBSCAN（Density-Based Spatial Clustering of Applications with Noise）</h4><p>基于局部密度估计（local density estimation），该算法允许对任意形状的数据进行聚类。</p>
<ol>
<li>对于每一个实例，算法计算一个小范围$\epsilon$ 内有多少实例，该范围叫做：实例的$\epsilon$-neighborhood</li>
<li>如果一个实例的$\epsilon$-neighborhood中有超过min_samples个实例，则该实例为core instance</li>
<li>所有在同一个core instance周围的实例属于同一个类，有可能一个core instance的周围有其他core instance。因此，一系列相邻的core instance形成一个cluster</li>
<li>任何实例，如果不是core instance，其相邻范围内也没有core instance，会被认为是异常数据</li>
</ol>
<p>当数据的可以被低密度区域分开时，DBSCAN变现得很好。</p>
<blockquote>
<p>sklearn中的DBSCAN没有predict方法，可以用DBSCAN的聚类中心（或者全部数据，或者除了异常数据的其他数据）训练其他算法（例如KNN），再预测新的数据。</p>
</blockquote>
<p><img src="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0915.png" alt="mls2 0915"></p>
<p>其实有两个数据需要被判定为异常值。</p>
<p>DBSCAN是一个简单但是强大的聚类算法，可以对不规则形状分布的数据进行聚类，并且只有两个超参数eps和min_samples，但如果类中密度变化很大，DBSCAN的表现可能没有那么好。</p>
<p><strong>计算复杂度</strong>：O(m log m)</p>
<h4 id="其他聚类算法"><a href="#其他聚类算法" class="headerlink" title="其他聚类算法"></a>其他聚类算法</h4><h5 id="层次聚类-Agglomerative-clustering"><a href="#层次聚类-Agglomerative-clustering" class="headerlink" title="层次聚类(Agglomerative clustering)"></a>层次聚类(Agglomerative clustering)</h5><p>是一种自底而上的层次聚类方法，它能够根据指定的相似度或距离定义计算出类之间的距离。</p>
<ol>
<li>将每一个元素单独定为一类</li>
<li>重复：每一轮都合并指定距离(对指定距离的理解很重要)最小的类</li>
<li>直到所有的元素都归为同一类</li>
</ol>
<blockquote>
<p>类似水面上的泡泡，会与相邻的泡泡合并</p>
</blockquote>
<p>依据对相似度（距离）的不同定义，将Agglomerative Clustering的聚类方法分为三种：Single-linkage, Complete-linkage和Group average.<br><strong>Single-linkage</strong>:要比较的距离为元素对之间的最小距离<br><strong>Complete-linkage</strong>:要比较的距离为元素对之间的最大距离<br><strong>Group average</strong>：要比较的距离为类之间的平均距离（平均距离的定义与计算：假设有A，B两个类，A中有n个元素，B中有m个元素。在A与B中各取一个元素，可得到他们之间的距离。将nm个这样的距离相加，得到距离和。最后距离和除以nm得到A，B两个类的平均距离。）</p>
<h5 id="BIRCH"><a href="#BIRCH" class="headerlink" title="BIRCH"></a>BIRCH</h5><p>Balanced Iterative Reducing and Clustering using Hierarchies，利用层次方法的平衡迭代规约和聚类，针对large database，并且比batch-K Means快</p>
<p>BIRCH算法利用了一个树结构来帮助我们快速的聚类，这个数结构类似于平衡B+树，一般将它称之为聚类特征树(Clustering Feature Tree，简称CF Tree)。这颗树的每一个节点是由若干个聚类特征(Clustering Feature，简称CF)组成。</p>
<p>可参考：<a href="https://www.cnblogs.com/pinard/p/6179132.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6179132.html</a></p>
<h5 id="Mean-Shift"><a href="#Mean-Shift" class="headerlink" title="Mean-Shift"></a>Mean-Shift</h5><p>也叫做均值漂移，在目标追踪中应用广泛。本身其实是一种<strong>基于密度</strong>的聚类算法。</p>
<p>主要思路是：中心向着密度高的地方移动，直到移动到local density maximum</p>
<ol>
<li>计算某一实例(A)与其周围半径R内的向量距离的平均值M，计算出该点下一步漂移（移动）的方向（A=M+A）。</li>
<li>重复上述过程，直到所有的点不再移动时，其与周围点形成一个类簇，</li>
<li>计算这个类簇与历史类簇的距离，满足小于阈值D即合并为同一个类簇，不满足则自身形成一个类簇。直到所有的数据点选取完毕。</li>
</ol>
<blockquote>
<p>当簇的内部具有密度不均匀时，Mean-shift倾向于将该簇分为几片</p>
<p>计算复杂度：$O(m^2)$,不适合大的数据集</p>
</blockquote>
<h5 id="Affinity-propagation"><a href="#Affinity-propagation" class="headerlink" title="Affinity propagation"></a>Affinity propagation</h5><p>AP(Affinity Propagation)通常被翻译为近邻传播算法或者亲和力传播算法。</p>
<p><strong>一种投票机制</strong>，每个实例对相似的实例投票，选出代表（representatives）一旦算法收敛，每个代表和其投票者聚成一簇。</p>
<blockquote>
<p>计算复杂度：$O(m^2)$,不适合大的数据集</p>
<p>AP算法的基本思想是将全部数据点都当作潜在的聚类中心(称之为exemplar)，然后数据点两两之间连线构成一个网络(相似度矩阵)，再通过网络中各条边的消息(responsibility和availability)传递计算出各样本的聚类中心。</p>
<p>聚类过程中，共有两种消息在各节点间传递，分别是吸引度（responsibility）和归属度（availability）。</p>
<p>AP算法通过迭代过程不断更新每一个点的吸引度和归属度，直到产生m个高质量的Exemplar（相当于质心），同时将其余的数据点分配到相应的聚类中。</p>
<p>可参考：<a href="https://www.biaodianfu.com/affinity-propagation.html" target="_blank" rel="noopener">https://www.biaodianfu.com/affinity-propagation.html</a></p>
</blockquote>
<h5 id="谱聚类"><a href="#谱聚类" class="headerlink" title="谱聚类"></a>谱聚类</h5><p>Spectral clustering，它的主要思想是把所有的数据看做空间中的点，这些点之间可以用边连接起来。距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高，通过对所有数据点组成的图进行切图，让切图后不同的子图间边权重和尽可能的低，而子图内的边权重和尽可能的高，从而达到聚类的目的。</p>
<h3 id="高斯混合模型-Gaussian-mixtures"><a href="#高斯混合模型-Gaussian-mixtures" class="headerlink" title="高斯混合模型 Gaussian mixtures"></a>高斯混合模型 Gaussian mixtures</h3><p><em>Gaussian mixture model</em> (GMM)：多个高斯分布的线性叠加能拟合非常复杂的密度函数；通过足够多的高斯分布叠加，并调节它们的均值，协方差矩阵，以及线性组合的系数，可以精确地逼近任意连续密度。</p>
<p> <code>GaussianMixture</code> class: 需要确定数据有几个高斯分布生成</p>
<blockquote>
<ol>
<li>对于每一个实例，从k个高斯分布（代表着k个簇）中随机选择，选择第j个高斯分布的概率$\phi^{(j)}$, 第i个实例选择的高斯分布记作$z^{(i)}$</li>
<li>如果$z^{(i)}=j$意味着第i个实例被分到了第j个簇中，实例$x^{(i)}~ N(\mu^{(j)},\sum^{(j)})$</li>
</ol>
</blockquote>
<p><img src="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0916.png" alt="mls2 0916"></p>
<h4 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h4><p>sklearn中的类<code>GaussianMixture</code>依据的是EM算法。</p>
<blockquote>
<p>和K-Means算法类似</p>
<ol>
<li>expectation step</li>
</ol>
<p>计算每个实例属于各个簇的概率（软分类）</p>
<ol>
<li>maximization step</li>
</ol>
<p>迭代</p>
<p>EM同样会收敛到不好的结果，所以需要多跑几次，保存最好的结果，<code>n_init=10</code></p>
</blockquote>
<p>GMM是一个生成模型，可以用来生成新的实例。</p>
<p>还可以预测每个实例周围的密度，<code>score_samples()</code>方法，计算实例的PDF（probability density function）的l对数。</p>
<p>对于高维数据，GMM可能陷入困难，需要对模型加以限制（减少自由度，限制分布的形状，大小和取向），通过超参数<code>`covariance_type</code> `</p>
<blockquote>
<p>取值可以是：sphere，diag，tied default：full</p>
</blockquote>
<p><strong>计算复杂度</strong>：取决于实例的个数m，数据的维度n，以及簇的个数k，还有对协方差矩阵的限制。</p>
<blockquote>
<p>diag or spherical：$O(kmn)$</p>
<p>tied or full: $O()kmn^2+kn^3$</p>
</blockquote>
<h4 id="异常值检测"><a href="#异常值检测" class="headerlink" title="异常值检测"></a>异常值检测</h4><p>GMM用于异常值检测（anomaly detection）也叫做outlier detection。</p>
<blockquote>
<p>处在GMM低密度区域的实例可以被认为是异常值。可以自己规定阈值。</p>
</blockquote>
<p>观察学习“正常（normal）”的数据是什么样子的，来检测出异常数据（应用场景：生产线产品部件的检测&amp;异常访问）</p>
<h4 id="新颖性检测"><a href="#新颖性检测" class="headerlink" title="新颖性检测"></a>新颖性检测</h4><p>novelty detection：检测新观察是否是异常值。 在这种情况下，异常值也被称为新颖点（a novelty）。是一种半监督的异常检测方法，在新颖性检测的背景下，新颖点可以形成密集的簇，只要它们处于训练数据的低密度区域中。</p>
<p>GMM尝试fit所有的数据，包括异常值，如果异常点过多，可以</p>
<blockquote>
<ol>
<li>去除最外侧的异常值，在cleaned-up的数据集上再次训练模型</li>
<li>使用更鲁棒的算法：EllipticEnvelope （适合于对数据的鲁棒协方差估计，从而将椭圆适配到中央数据点，忽略中央模式之外的点。）</li>
</ol>
</blockquote>
<h4 id="如何确定cluster的个数"><a href="#如何确定cluster的个数" class="headerlink" title="如何确定cluster的个数"></a>如何确定cluster的个数</h4><p>对于大小不一致或者非球型分布的数据，K-Means中用来确定cluster数目的inertia和轮廓系数都不可行。</p>
<p>我们通过<strong>最小化理论信息标准（theoretical information criterion），如赤池信息量（<em>Akaike information criterion</em> ，AIC）,贝叶斯信息量（BIC，<em>Bayesian information criterion</em>）</strong></p>
<p>定义：(m是实例的个数，p是模型的参数，$\hat L$ 是模型似然函数（likelihood function）的最大值)</p>
<p>$BIC\ = \ \log(m)p-2\ \log(\hat L) \\ AIC  \ = 2p-2\ \log(\hat L)$</p>
<p>BIC和AIC对参数多模型加上penalty，一般情况下，BIC选择的模型比AIC简单，但是不如AIC的模型和数据吻合得好。</p>
<p><strong>似然函数</strong></p>
<p>概率（probability）是指：给定模型的参数，输出x的可信程度。（下图水平黑线，和PDF图）</p>
<p>似然（likelihood）是指：一直输出x的前提下，模型的参数有多可信。（下图垂直蓝色线，和似然函数图）</p>
<p><img src="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0920.png" alt="mls2 0920"></p>
<h4 id="Bayesian-Gaussian-Mixture-Models"><a href="#Bayesian-Gaussian-Mixture-Models" class="headerlink" title="Bayesian Gaussian Mixture Models"></a>Bayesian Gaussian Mixture Models</h4><p>可以赋予不重要的簇权重0，可以通过n_components预设簇的最大值。</p>
<p><img src="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0922.png" alt="mls2 0922"></p>
<p>Stick-Breaking Process (SBP)</p>
<p>Wishart distribution</p>
<p><strong>贝叶斯定理</strong></p>
<p>$p(z|X)=posterior=\frac{likelihood\times prior}{evidence}=\frac{p(X|z)p(z)}{p(X)}$</p>
<p>$p(X)=\int p(X|z)p(z)dz$</p>
<h3 id="其他处理异常值和新颖性检测的算法"><a href="#其他处理异常值和新颖性检测的算法" class="headerlink" title="其他处理异常值和新颖性检测的算法"></a>其他处理异常值和新颖性检测的算法</h3><h4 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h4><p>数据重建，正常值重建后与原始数据差距小，而异常值重建后与异常值差距大</p>
<h4 id="Fast-MCD"><a href="#Fast-MCD" class="headerlink" title="Fast-MCD"></a>Fast-MCD</h4><p>minimum covariance determinant，</p>
<p>认为正常值（inliers）是由单一的高斯模型产生的，不是由该单一高斯模型产生的数据点被认为是outliers</p>
<h4 id="Isolation-Forest"><a href="#Isolation-Forest" class="headerlink" title="Isolation Forest"></a>Isolation Forest</h4><p>孤立森林： 是一个基于Ensemble的快速异常检测方法，具有线性时间复杂度和高精准度，是符合大数据处理要求的state-of-the-art算法。适用于<strong>连续数据（Continuous numerical data）的异常检测</strong>，将异常定义为“容易被孤立的离群点 (more likely to be separated)”——可以理解为分布稀疏且离密度高的群体较远的点。</p>
<p>构建随机森林，其中每一个决策树都是随机的，在每个节点，随机的选择特征和阈值，将数据分为两个部分。</p>
<p>适用于高维数据</p>
<h4 id="Local-Outlier-Factoer（LOF）"><a href="#Local-Outlier-Factoer（LOF）" class="headerlink" title="Local Outlier Factoer（LOF）"></a>Local Outlier Factoer（LOF）</h4><p>一个样本点周围的样本点所处位置的平均密度比上该样本点所在位置的密度。比值越大于1，则该点所在位置的密度越小于其周围样本所在位置的密度，这个点就越有可能是异常点。</p>
<h4 id="One-class-SVM"><a href="#One-class-SVM" class="headerlink" title="One-class SVM"></a>One-class SVM</h4><p>单分类算法，寻找一个超平面将样本中的正例圈出来，预测就是用这个超平面做决策，在圈内的样本就认为是正样本。由于<strong>核函数</strong>计算比较耗时，在海量数据的场景用的并不多；</p>
<p><strong>适用于新颖性检测</strong></p>
<p><img src="https://img2018.cnblogs.com/blog/1226410/201904/1226410-20190424112057869-1957699378.png" alt="img"></p>
<h3 id="密度估计（density-estimation）"><a href="#密度估计（density-estimation）" class="headerlink" title="密度估计（density estimation）"></a>密度估计（density estimation）</h3><p>用来估计随机过程的概率密度函数（probability density function（PDF））（应用场景：异常检测（密度低的地方异常的概率大），数据分析和可视化）</p>
<h3 id="练习-3"><a href="#练习-3" class="headerlink" title="练习"></a>练习</h3><ol>
<li><p>How would you define clustering? Can you name a few clustering algorithms?</p>
<blockquote>
<p>聚类是一种无监督学习，将具有相似特征的实例聚集在一起。</p>
<p>K-Means, DBSCAN，层次聚类，BRICH，Mean-shift，Affinity propagation和谱聚类</p>
</blockquote>
</li>
<li><p>What are some of the main applications of clustering algorithms?</p>
<blockquote>
<p>半监督学习</p>
<p>图像分割</p>
<p>数据预处理</p>
<p>异常值检测</p>
<p>新颖性检测</p>
</blockquote>
</li>
<li><p>Describe two techniques to select the right number of clusters when using K-Means.</p>
<blockquote>
<ol>
<li>画出k-inertia图像，选择肘部</li>
<li>画出k-轮廓系数图像，选择最高点</li>
</ol>
</blockquote>
</li>
<li><p>What is label propagation? Why would you implement it, and how?</p>
<blockquote>
<p>因为打标签是非常耗时的，所以实际情况下，数据集中可能只有很少一部分有标签，大部分数据都没有标签，标签传递可以得到更多的带标签的数据，可以进行监督学习（这个过程就是非监督学习）</p>
<p>当数据中仅有少部分具有标签的数据时，可以先进行聚类，然后将标签打到具有相似特征的实例上（离聚类中心最近的有标签数据）。</p>
</blockquote>
</li>
<li><p>Can you name two clustering algorithms that can scale to large datasets? And two that look for regions of high density?</p>
<blockquote>
<p>K-Means和BRICH可以用于大的数据集</p>
<p>DBSCAN和Mean-shift（中心向着密度高的方向移动）寻找区域内高密度的位置</p>
</blockquote>
</li>
<li><p>Can you think of a use case where active learning would be useful? How would you implement it?</p>
<blockquote>
<p>不是随机的选择数据打标签，而是人类专家参与算法的学习，为特定的实例提供标签。</p>
<p>不确定采样</p>
</blockquote>
</li>
<li><p>What is the difference between anomaly detection and novelty detection?</p>
<blockquote>
<p>异常值检测中：模型在包含异常值的数据集上训练，（孤立森林）</p>
<p>新颖性检测中：模型在一个clean的数据及上训练，（单分类-SVM）</p>
</blockquote>
</li>
<li><p>What is a Gaussian mixture? What tasks can you use it for?</p>
<blockquote>
<p>GMM是一种概率模型，假设实例是由若干个高斯模型混合生成的。换句话说，假设数据属于若干椭圆类，但大小，方向，形状和密度都不确定。</p>
<p>可以用来密度检测，异常值检测，聚类。</p>
</blockquote>
</li>
<li><p>Can you name two techniques to find the right number of clusters when using a Gaussian mixture model?</p>
<blockquote>
<p>画出k-AIC或者k-BIC的图像，选择使AIC或者BIC数值最小的聚类个数。</p>
</blockquote>
</li>
<li><p>The classic Olivetti faces dataset contains 400 grayscale 64 × 64–pixel images of faces. Each image is flattened to a 1D vector of size 4,096. 40 different people were photographed (10 times each), and the usual task is to train a model that can predict which person is represented in each picture. Load the dataset using the <code>sklearn.datasets.fetch_olivetti_faces()</code> function, then split it into a training set, a validation set, and a test set (note that the dataset is already scaled between 0 and 1). Since the dataset is quite small, you probably want to use stratified sampling to ensure that there are the same number of images per person in each set. Next, cluster the images using K-Means, and ensure that you have a good number of clusters (using one of the techniques discussed in this chapter). Visualize the clusters: do you see similar faces in each cluster?</p>
</li>
<li><p>Continuing with the Olivetti faces dataset, train a classifier to predict which person is represented in each picture, and evaluate it on the validation set. Next, use K-Means as a dimensionality reduction tool, and train a classifier on the reduced set. Search for the number of clusters that allows the classifier to get the best performance: what performance can you reach? What if you append the features from the reduced set to the original features (again, searching for the best number of clusters)?</p>
</li>
<li><p>Train a Gaussian mixture model on the Olivetti faces dataset. To speed up the algorithm, you should probably reduce the dataset’s dimensionality (e.g., use PCA, preserving 99% of the variance). Use the model to generate some new faces (using the <code>sample()</code> method), and visualize them (if you used PCA, you will need to use its <code>inverse_transform()</code> method). Try to modify some images (e.g., rotate, flip, darken) and see if the model can detect the anomalies (i.e., compare the output of the <code>score_samples()</code> method for normal images and for anomalies).</p>
</li>
<li><p>Some dimensionality reduction techniques can also be used for anomaly detection. For example, take the Olivetti faces dataset and reduce it with PCA, preserving 99% of the variance. Then compute the reconstruction error for each image. Next, take some of the modified images you built in the previous exercise, and look at their reconstruction error: notice how much larger the reconstruction error is. If you plot a reconstructed image, you will see why: it tries to reconstruct a normal face.</p>
</li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/sklearn/" rel="tag"># sklearn</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/08/23/%E3%80%8A%E5%88%A9%E7%94%A8Python%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%8B%E5%AD%A6%E4%B9%A0/" rel="prev" title="《利用Python进行数据分析》学习">
      <i class="fa fa-chevron-left"></i> 《利用Python进行数据分析》学习
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/08/25/%E5%A6%82%E4%BD%95%E5%9C%A8Jupyter%20Notebook%E4%B8%AD%E4%BD%BF%E7%94%A8Python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%EF%BC%9F/" rel="next" title="如何在Jupyter Notebook中使用Python虚拟环境？">
      如何在Jupyter Notebook中使用Python虚拟环境？ <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#第01章-机器学习概览"><span class="nav-text">第01章 机器学习概览</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#问题："><span class="nav-text">问题：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第02章-一个完整的机器学习项目"><span class="nav-text">第02章 一个完整的机器学习项目</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#特点：检错能力极强，开销小等"><span class="nav-text">特点：检错能力极强，开销小等</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#本质"><span class="nav-text">本质</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CRC-校验的基本过程"><span class="nav-text">CRC 校验的基本过程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第03章-分类"><span class="nav-text">第03章 分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#练习"><span class="nav-text">练习</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第04章-训练模型"><span class="nav-text">第04章 训练模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#正态方程"><span class="nav-text">正态方程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降法"><span class="nav-text">梯度下降法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#batch-GD"><span class="nav-text">batch-GD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#stochastic-gradient-descent"><span class="nav-text">stochastic gradient descent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mini-batch-GD"><span class="nav-text">mini-batch GD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多项式回归"><span class="nav-text">多项式回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#学习曲线"><span class="nav-text">学习曲线</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性模型的正则化"><span class="nav-text">线性模型的正则化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Exercise"><span class="nav-text">Exercise</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第05章-支持向量机"><span class="nav-text">第05章 支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#线性支持向量机分类"><span class="nav-text">线性支持向量机分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#非线性支持向量机分类"><span class="nav-text">非线性支持向量机分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM回归"><span class="nav-text">SVM回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#背后机制"><span class="nav-text">背后机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Exercise-1"><span class="nav-text">Exercise</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第06章-决策树"><span class="nav-text">第06章 决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树的训练和可视化"><span class="nav-text">决策树的训练和可视化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#估计分类概率"><span class="nav-text">估计分类概率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CART训练算法"><span class="nav-text">CART训练算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#NP完全问题"><span class="nav-text">NP完全问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#计算复杂度"><span class="nav-text">计算复杂度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基尼不纯度和信息熵"><span class="nav-text">基尼不纯度和信息熵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则化超参数"><span class="nav-text">正则化超参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#回归"><span class="nav-text">回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#不稳定性"><span class="nav-text">不稳定性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#exercise"><span class="nav-text">exercise</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第07章-集成学习和随机森林"><span class="nav-text">第07章 集成学习和随机森林</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#投票分类"><span class="nav-text">投票分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bagging"><span class="nav-text">Bagging</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Out-of-bag"><span class="nav-text">Out-of-bag</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#随机贴片与随机子空间"><span class="nav-text">随机贴片与随机子空间</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机森林（一种bagging模型）"><span class="nav-text">随机森林（一种bagging模型）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#极端随机树"><span class="nav-text">极端随机树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#特征重要度"><span class="nav-text">特征重要度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#提升"><span class="nav-text">提升</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Adaboost"><span class="nav-text">Adaboost</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度提升"><span class="nav-text">梯度提升</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#极端梯度提升（XGBoost）"><span class="nav-text">极端梯度提升（XGBoost）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stacking"><span class="nav-text">Stacking</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#练习-1"><span class="nav-text">练习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第08章-降维"><span class="nav-text">第08章 降维</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#维数灾难（curse-of-dimentionality）"><span class="nav-text">维数灾难（curse of dimentionality）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#降维的主要方法"><span class="nav-text">降维的主要方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#投影（Projection）"><span class="nav-text">投影（Projection）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#流形学习（Manifold-Learning）"><span class="nav-text">流形学习（Manifold Learning）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#降维技术"><span class="nav-text">降维技术</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#主成分分析（PCA）"><span class="nav-text">主成分分析（PCA）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#核-PCA（Kernel-PCA）"><span class="nav-text">核 PCA（Kernel PCA）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#局部线性嵌入（LLE）"><span class="nav-text">局部线性嵌入（LLE）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#其他降维方法"><span class="nav-text">其他降维方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#练习-2"><span class="nav-text">练习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第09章-非监督学习"><span class="nav-text">第09章 非监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#聚类（clustering）"><span class="nav-text">聚类（clustering）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-Means"><span class="nav-text">K-Means</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#加速K-Means算法"><span class="nav-text">加速K-Means算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mini-batch-K-Means算法"><span class="nav-text">mini-batch K-Means算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#确定最佳类的个数"><span class="nav-text">确定最佳类的个数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#silhouette-score（平均轮廓系数）"><span class="nav-text">silhouette score（平均轮廓系数）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-Means的局限性"><span class="nav-text">K-Means的局限性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-Means的具体应用"><span class="nav-text">K-Means的具体应用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#主动学习-active-learning"><span class="nav-text">主动学习(active learning)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#KNN和K-Means的区别"><span class="nav-text">KNN和K-Means的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DBSCAN（Density-Based-Spatial-Clustering-of-Applications-with-Noise）"><span class="nav-text">DBSCAN（Density-Based Spatial Clustering of Applications with Noise）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#其他聚类算法"><span class="nav-text">其他聚类算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#高斯混合模型-Gaussian-mixtures"><span class="nav-text">高斯混合模型 Gaussian mixtures</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#EM算法"><span class="nav-text">EM算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#异常值检测"><span class="nav-text">异常值检测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#新颖性检测"><span class="nav-text">新颖性检测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何确定cluster的个数"><span class="nav-text">如何确定cluster的个数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bayesian-Gaussian-Mixture-Models"><span class="nav-text">Bayesian Gaussian Mixture Models</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其他处理异常值和新颖性检测的算法"><span class="nav-text">其他处理异常值和新颖性检测的算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#PCA"><span class="nav-text">PCA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Fast-MCD"><span class="nav-text">Fast-MCD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Isolation-Forest"><span class="nav-text">Isolation Forest</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Local-Outlier-Factoer（LOF）"><span class="nav-text">Local Outlier Factoer（LOF）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#One-class-SVM"><span class="nav-text">One-class SVM</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#密度估计（density-estimation）"><span class="nav-text">密度估计（density estimation）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#练习-3"><span class="nav-text">练习</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="QQAI"
      src="/images/header.jpg">
  <p class="site-author-name" itemprop="name">QQAI</p>
  <div class="site-description" itemprop="description">Home is behind, the world ahead</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">34</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QQAI</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.1
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.1
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">
    MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        extensions: ['[mhchem]/mhchem.js'],
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
